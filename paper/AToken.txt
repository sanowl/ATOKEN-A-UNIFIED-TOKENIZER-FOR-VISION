5
2
0
2

p
e
S
7
1

]

V
C
.
s
c
[

1
v
6
7
4
4
1
.
9
0
5
2
:
v
i
X
r
a

Preprint

ATOKEN: A UNIFIED TOKENIZER FOR VISION

Jiasen Lu‚àó Liangchen Song‚àó Mingze Xu Byeongjoo Ahn

Yanjun Wang Chen Chen Afshin Dehghan Yinfei Yang

Apple

ABSTRACT

We present ATOKEN, the first unified visual tokenizer that achieves both high-
fidelity reconstruction and semantic understanding across images, videos, and
3D assets. Unlike existing tokenizers that specialize in either reconstruction or
understanding for single modalities, ATOKEN encodes these diverse visual in-
puts into a shared 4D latent space, unifying both tasks and modalities in a single
framework. Specifically, we introduce a pure transformer architecture with 4D
rotary position embeddings to process visual inputs of arbitrary resolutions and
temporal durations. To ensure stable training, we introduce an adversarial-free
training objective that combines perceptual and Gram matrix losses, achieving
state-of-the-art reconstruction quality. By employing a progressive training cur-
riculum, ATOKEN gradually expands from single images, videos, and 3D, and
supports both continuous and discrete latent tokens. ATOKEN achieves 0.21 rFID
with 82.2% ImageNet accuracy for images, 3.01 rFVD with 32.6% MSRVTT re-
trieval for videos, and 28.19 PSNR with 90.9% classification accuracy for 3D.
In downstream applications, ATOKEN enables both visual generation tasks (e.g.,
image generation with continuous and discrete tokens, text-to-video generation,
image-to-3D synthesis) and understanding tasks (e.g., multimodal LLMs), achiev-
ing competitive performance across all benchmarks. These results shed light on
the next-generation multimodal AI systems built upon unified visual tokenization.

1

INTRODUCTION

Large Language Models (LLMs) (Chowdhery et al., 2023; Achiam et al., 2023; Touvron et al., 2023;
Team et al., 2023; Guo et al., 2025) have achieved unprecedented generalization, with single models
handling coding, reasoning, translation, and numerous other tasks that previously required special-
ized systems. This versatility largely stems from transformer architectures and simple tokenizers,
such as BPE (Sennrich et al., 2015), which convert all text types ‚Äì code, documents, tables, and
multiple languages ‚Äì into a unified token space. This shared representation enables efficient scaling
and seamless knowledge transfer across language tasks.

In contrast, visual representations remain fragmented due to inherent complexities. Unlike text‚Äôs
discrete symbolic nature, visual tasks demand distinct levels of abstraction: generation requires
tokenizers that preserve low-level visual details for reconstruction, while understanding requires en-
coders that extract high-level semantic features through text alignment. Moreover, visual data exists
in disparate formats: 2D grids for images, temporal sequences for videos, and varied 3D represen-
tations (e.g., meshes, voxels, and Gaussian splats) (Mescheder et al., 2019; Achlioptas et al., 2018;
Mildenhall et al., 2021; Kerbl et al., 2023). Without a shared representation, vision systems remain
fundamentally limited, unable to achieve the generalization and transfer learning that characterizes
modern language models.

Despite recent progress, unified visual tokenizers face three fundamental challenges. First, exist-
ing approaches optimize for either reconstruction or understanding, but not both: visual encoders
(Radford et al., 2021; Zhai et al., 2023; Bolya et al., 2025) achieve semantic alignment but lack

*Leading authors, equal contribution. Description of each author‚Äôs contribution is available in Appendix A.

Corresponding to Jiasen Lu.

1

 
 
 
 
 
 
Preprint

Figure 1: Illustration of our method on different visual modalities. Given images, videos, and 3D assets,
ATOKEN leverages a shared 4D latent space (left) to produce high-fidelity reconstructions (middle: zoomed
regions with red boxes for images, temporal frames for videos, multiple viewpoints for 3D) while preserving
strong semantic understanding (right: showing text-aligned representations for zero-shot text retrieval).

pixel-level detail, while VAE-based tokenizers (Esser et al., 2020; Rombach et al., 2022; Polyak
et al., 2024; Yu et al., 2022b) preserve visual details but lack semantic understanding. Second, ar-
chitectural choices create different limitations: convolutional tokenizers exhibit diminishing returns
when scaling model parameters (Xiong et al., 2025), while transformer tokenizers (Yu et al., 2021;
Wang et al., 2024b; Hansen-Estruch et al., 2025) achieve better scaling but suffer from severe adver-
sarial training instabilities. Third, recent unification efforts remain limited to images (Deng et al.,
2025; Wu et al., 2024c; Ma et al., 2025a), while video and 3D modalities remain unexplored.

In this paper, we present ATOKEN, a general-purpose visual tokenizer that achieves high-fidelity
reconstruction and rich semantic understanding across images, videos, and 3D. Our model learns
a unified representation that captures both fine-grained visual details and high-level semantics, ac-
cessible through progressive encoding: semantic embeddings for understanding, low-dimensional
continuous latents for generation, and discrete tokens via quantization. This design enables the next
generation of multimodal systems that seamlessly handle both understanding and generation across
all visual modalities, as shown in Figure 1.

To address format discrepancies across visual modalities, we introduce a sparse 4D representation
where each modality naturally occupies different subspaces: images as 2D slices, videos as temporal
stacks, and 3D assets as surface voxels extracted from multi-view renderings (Xiang et al., 2024).
We implement this through a pure transformer architecture with space-time patch embeddings and
4D Rotary Position Embeddings (RoPE), enabling efficient scaling and joint modeling across all
modalities while maintaining native resolution and temporal length processing.

To overcome training instabilities that affect transformer-based visual tokenizers, we develop an
adversarial-free loss combining perceptual and Gram matrix terms. This approach achieves state-
of-the-art reconstruction quality while maintaining stable, scalable training. We further introduce
a progressive curriculum that builds capabilities incrementally: starting from a pretrained vision
encoder, jointly optimizing reconstruction and understanding for images, extending to videos and
3D data, with optional quantization for discrete tokens. Surprisingly, this curriculum reveals that
multimodal training can enhance rather than compromise single-modality performance ‚Äì our final
model achieves better image reconstruction than earlier image-only stages while maintaining strong
semantic understanding.

ATOKEN demonstrates significant advances in both scalability and performance. The model natively
processes arbitrary resolutions and time duration, and accelerates inference through KV-caching
mechanisms. To validate its effectiveness, we conduct comprehensive evaluations across three di-
mensions: reconstruction quality, semantic understanding, and downstream applications. These
experiments confirm that ATOKEN achieves competitive or state-of-the-art performance across all
modalities while maintaining computational efficiency.

The key contributions of ATOKEN can be summarized as follows:

‚Ä¢ First unified visual tokenizer across modalities and tasks: We present the first tokenizer that
achieves high-fidelity reconstruction and semantic understanding for images, videos, and 3D as-
sets, supporting both continuous and discrete representations within a single framework.

2

yxtyxyxzImageVideo3DAerial view of Japanese city ‚Ä¶Asian city at night with ‚Ä¶Ocean waves crashing on ‚Ä¶Desert landscape with sand ‚Ä¶‚Ä¶Squirrel running to the right ‚Ä¶Bird flying overhead ‚Ä¶‚Ä¶3D rendered orange fox ‚Ä¶ ‚Ä¶Blue elephant walking ‚Ä¶‚Ä¶‚Ä¶‚Ä¶ReconstructionUnderstanding4DRepresentationPreprint

‚Ä¢ Sparse 4D representation with pure transformer architecture: We introduce a unified 4D la-
tent space where different modalities naturally occupy respective subspaces, implemented through
space-time patch embeddings and 4D RoPE that enable native resolution and temporal processing.
‚Ä¢ Adversarial-free training for stable optimization: We demonstrate that combining perceptual
and Gram matrix losses achieves state-of-the-art reconstruction quality without adversarial train-
ing, overcoming instabilities that challenge transformer-based visual tokenizers.

‚Ä¢ Progressive curriculum across modalities: Our four-stage training strategy enables stable learn-
ing while maintaining strong performance, with image reconstruction quality preserved or im-
proved when video and 3D capabilities are added alongside semantic understanding.

‚Ä¢ Strong empirical validation across downstream applications: ATOKEN achieves competitive
performance across all modalities and enables diverse applications from multimodal LLMs to
image-to-3D generation, validating its effectiveness as a universal visual foundation.

2 BACKGROUND

Visual tokenization transforms raw visual data into compact representations suitable for both under-
standing and generation tasks. However, existing approaches remain fragmented across modalities
and task objectives, unable to achieve the versatility seen in language models. Table 1 summa-
rizes the landscape of visual tokenizers across three key dimensions: task specialization, modality
fragmentation, and architectural trade-offs. A comprehensive review of related work is in Section 6.

Task Specialization. Current visual tokenizers fall into two distinct categories based on their opti-
mization objectives. Reconstruction methods like SD-VAE (Rombach et al., 2022), VQGAN (Esser
et al., 2020), GigaTok (Xiong et al., 2025), and Cosmos (Agarwal et al., 2025) excel at compressing
visual data for generation tasks but cannot extract semantic features for understanding. Conversely,
understanding-centric visual encoders such as CLIP (Radford et al., 2021), SigLIP2 (Tschannen
et al., 2025), and VideoPrism (Zhao et al., 2024) produce rich semantic representations but cannot
reconstruct the original visual content. Only recent works VILA-U (Wu et al., 2024c) and UniTok
(Ma et al., 2025a) attempt both tasks simultaneously, though they remain limited to images. This
divide prevents building visual models that excel at both generation and understanding.

Modality Fragmentation. Beyond task specialization, visual tokenizers are limited to specific
modalities. While most video tokenizers naturally handle images as single-frame videos (e.g., TAE
(Polyak et al., 2024), Hunyuan (Kong et al., 2024), OmniTokenizer (Wang et al., 2024b)), they
cannot process 3D data. Conversely, 3D tokenizers like Trellis-SLAT (Xiang et al., 2024) are re-
stricted to 3D-only data, unable to leverage the massive image and video data for pretraining. Un-
derstanding tasks face similar constraints: image encoders process videos frame-by-frame without
temporal compression, while dedicated video encoders (Zhao et al., 2024; Wang et al., 2022b) lack
image-specific optimizations. No existing method provides comprehensive coverage across all three
modalities for both reconstruction and understanding tasks.

Architectural Trade-offs. Key design trade-offs emerge across methods: (1) Architecture: Un-
derstanding encoders use transformers, while reconstruction tokenizers favor convolutional archi-
tectures (e.g., SD-VAE (Rombach et al., 2022)). Recent works explore hybrid (e.g., GigaTok (Xiong
et al., 2025)) and pure transformer approaches (e.g., ViTok (Hansen-Estruch et al., 2025)), though
the latter suffer from adversarial training instabilities. (2) Token representation: Methods choose
between discrete tokens for LLM compatibility (e.g., VQGAN (Esser et al., 2020)) or continu-
ous tokens for reconstruction quality (e.g., TAE (Polyak et al., 2024)), with few supporting both.
(3) Resolution handling: Convolutional architectures naturally handle arbitrary resolutions, while
among transformer-based approaches, only SigLIP2 (Tschannen et al., 2025) supports native reso-
lution processing. (4) Training objectives: GAN-based training dominates reconstruction tokenizers
for quality despite instabilities. Trellis-SLAT (Xiang et al., 2024) avoids adversarial training as 3D
assets lack the fine detail of real images and videos.

These limitations motivate ATOKEN, which unifies reconstruction and understanding across images,
videos, and 3D within a single transformer framework. As shown in Table 1, ATOKEN is the only
method providing full coverage ‚Äì both tasks, all modalities, both token types ‚Äì while achieving
training stability through adversarial-free optimization.

3

Preprint

Table 1: Comparison between existing visual tokenizers and AToken. We categorize methods by task
capabilities (reconstruction, understanding, or both) and evaluate their modality coverage, architectural choices,
token representations, and key features. ATOKEN is the only method providing support across all dimensions.

Method

Encoder
Arch.

Decoder
Arch.

Reconstruction Only

SD-VAE
VQGAN
GigaTok
OmniTokenizer
MAGVIT-v2
Cosmos
ViTok
TAE
Hunyuan
Wan
Trellis-SLAT

Conv
Conv
Conv
Conv
Hybrid Hybrid
Trans
Trans
Conv
Conv
Conv
Conv
Trans
Trans
Conv
Conv
Conv
Conv
Conv
Conv
Trans
Trans

Understanding Only

SigLIP2
PE
VideoPrism
InternVideo

Trans
Trans
Trans
Trans

-
-
-
-

Reconstruction & Understanding

VILA-U
UniTok

ATOKEN

Trans
Trans

Trans

Conv
Hybrid

Trans

3 MODEL

Reconstruction

Understanding

Image

Video

3D

Image

Video

3D

Discrete
Token

Cont.
Token

GAN
Free

Temporal
Comp.

Native
Res.

‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
‚úó

‚úó
‚úó
‚úó
‚úó

‚úì
‚úì

‚úì

‚úó
‚úó
‚úó
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
‚úó

‚úó
‚úó
‚úó
‚úó

‚úó
‚úó

‚úì

‚úó
‚úó
‚úó
‚úó
‚úó
‚úó
‚úó
‚úó
‚úó
‚úó
‚úì

‚úó
‚úó
‚úó
‚úó

‚úó
‚úó

‚úì

‚úó
‚úó
‚úó
‚úó
‚úó
‚úó
‚úó
‚úó
‚úó
‚úó
‚úó

‚úì
‚úì
‚úì
‚úì

‚úì
‚úì

‚úì

‚úó
‚úó
‚úó
‚úó
‚úó
‚úó
‚úó
‚úó
‚úó
‚úó
‚úó

‚úì
‚úì
‚úì
‚úì

‚úì
‚úì

‚úì

‚úó
‚úó
‚úó
‚úó
‚úó
‚úó
‚úó
‚úó
‚úó
‚úó
‚úó

‚úó
‚úó
‚úó
‚úó

‚úó
‚úó

‚úì

‚úó
‚úì
‚úì
‚úì
‚úì
‚úì
‚úó
‚úó
‚úó
‚úó
‚úó

-
-
-
-

‚úì
‚úì

‚úì

‚úì
‚úó
‚úó
‚úì
‚úó
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì

-
-
-
-

‚úó
‚úó

‚úì

‚úó
‚úó
‚úó
‚úó
‚úó
‚úó
‚úó
‚úó
‚úó
‚úó
‚úì

-
-
-
-

‚úó
‚úó

‚úì

‚úó
‚úó
‚úó
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
‚úì
‚úó

‚úó
‚úó
‚úó
‚úì

‚úó
‚úó

‚úì

-
-
‚úó
‚úó
-
-
‚úó
-
-
-
-

‚úì
‚úó
‚úó
‚úó

‚úó
‚úó

‚úì

This section presents ATOKEN‚Äôs architecture and training methodology. We first present our unified
4D representation that bridges all visual modalities (Section 3.1) and the pure transformer architec-
ture that processes these representations (Section 3.2). We then describe our adversarial-free training
objectives for stable optimization (Section 3.3) and our progressive curriculum that enables effective
multimodal learning (Section 3.4), followed by implementation details (Section 3.5).

3.1 UNIFIED LATENT REPRESENTATION

Unified Modalities ‚Äì Image, Video and 3D. Our central insight is that all visual modalities can be
represented within a shared 4D space. As illustrated in Figure 2, we process each modality through
space-time patchification to produce sets of feature-coordinate pairs:

z = {(zi, pi)}L

i=1,

zi ‚àà RC, pi ‚àà {0, 1, . . . , N ‚àí 1}4

(1)

where zi represents the latent feature at position pi = [t, x, y, z] in 4D space (temporal and spatial
coordinates), with N defining the resolution along each axis and L the number of active locations.

This sparse representation unifies all modalities by activating only their relevant dimensions: images
occupy the (x, y) plane at t = z = 0, videos extend along the temporal axis with z = 0, and 3D
assets as surface voxels in (x, y, z) space with t = 0. For 3D assets, we adapt Trellis-SLAT (Xiang
et al., 2024) by rendering multi-view images from spherically sampled cameras, applying our unified
patchification, then aggregating features into voxel space (detailed in Section 3.2). This approach
enables a single encoder E to process all modalities without architectural modifications.

Note that the (x, y, z) coordinates serve different purposes across modalities: in 3D, they represent
actual entity occupancy physical locations, while in images and videos, they function as grid in-
dices. We can conceptualize this as placing a monitor within 4D space and encoding its displayed
content for image and video data. This dual interpretation of coordinates does not compromise
generalization, thanks to the use of 4D RoPE, which we describe in detail in following sections.

Unified Tasks ‚Äì Reconstruction and Understanding. From the unified structured latents z =
{(zi, pi)}, we extract representations for both reconstruction and understanding through com-
plementary projections. For reconstruction, we project each latent to a lower-dimensional space
zr = Wr(z) with KL regularization (Rombach et al., 2022), optionally applying FSQ (Mentzer
et al., 2023) for discrete codes Àúzr = FSQ(zr). The decoder DŒ∏ then reconstructs the input from

4

Preprint

Figure 2: Overview of our method. All modalities undergo unified space-time patchification and encoding
into sparse 4D latents, which support both reconstruction through modality-specific decoders and understanding
through attention pooling and text alignment. The architecture jointly optimizes reconstruction and understand-
ing losses, maintaining sparse structured representations throughout for efficient multimodal processing.

these latents. For understanding, we aggregate latents via attention pooling (Radford et al., 2021;
Tschannen et al., 2025) into a global representation ¬Øz, which is projected to zs = Ws( ¬Øz) for align-
ment with text embeddings. This dual projection design allows joint optimization without archi-
tectural duplication ‚Äì the same encoded features z support both pixel-level reconstruction through
individual latents and semantic understanding through their aggregation.

3.2 TRANSFORMER BASED ARCHITECTURE

Unified Space-Time Patch Embedding. We employ a unified patchification scheme that enables
all modalities to share the same encoder. Given an input x ‚àà RT √óH√óW √ó3, we partition it into
non-overlapping space-time patches of size t √ó p √ó p. For images (T = 1), we apply temporal
zero-padding to create t-frame patches, ensuring consistent dimensions across modalities. Videos
are directly partitioned along both spatial and temporal dimensions.

For 3D assets, we adapt Trellis-SLAT (Xiang et al., 2024) to our unified pipeline. As shown in
Figure 3, we render multi-view images from spherically sampled cameras and apply our standard
space-time patchification. Each voxel in a 643 grid is back-projected to gather and average patch
features from relevant views. Unlike Xiang et al. (2024), which uses DINOv2 features, we achieve
comparable quality using our unified patch representation.

All patch features ‚Äì whether from images, videos, or aggregated 3D views ‚Äì are then flattened and
passed through a shared linear layer to produce the initial embeddings for the transformer encoder.

Sparse Transformer Encoder and Decoder. We employ a unified transformer architecture for
both encoder and decoder, as illustrated in Figure 2. Both components process sparse structured
representations ‚Äì sets of feature-position pairs rather than dense grids ‚Äì enabling efficient handling
of all modalities with native support for arbitrary resolutions and temporal lengths.

Our encoder E extends the pretrained SigLIP2 vision tower (Tschannen et al., 2025) from 2D images
to 4D representations through two modifications. First, we generalize patch embedding to space-
time blocks of size t √ó p √ó p, with zero-initialized temporal weights preserving the original image
features. Second, we augment SigLIP2‚Äôs learnable 2D position embeddings with 4D RoPE (Lu et al.,
2024a) applied in every attention layer, providing relative position awareness across (t, x, y, z) di-
mensions. This design maintains SigLIP2‚Äôs semantic priors and resolution flexibility while enabling
unified processing across modalities.

The decoder D shares the encoder‚Äôs transformer architecture but is trained from scratch for recon-
struction. It maps structured latents back to visual outputs through task-specific heads. For images
and videos, we decode directly to pixel space:
DP : {(zi, pi)}L

i=1 ‚Üí x ‚àà RT √óH√óW √ó3

(2)

treating images as single-frame videos (T = 1) and discarding temporal padding following (Polyak
et al., 2024). For 3D assets, we first decode to pixel-space features, then apply an additional layer to

5

SigLIP2 Vision EncoderImageVideo3DSparse TransformerEncoderSigLIP2 Text EncoderThis is an image of a cute catTextSpace-Time PatchifyMultiview aggregation (3D)xyxyxytzLinear Proj.Sparse TransformerDecoderSigmoid Loss (Video / 3D ‚Äì Text) Distillation Loss (Image ‚Äì Text) Reconstruction Loss + LPIPS + CLIP Perceptual Loss + Gram Loss KL Loss4D Rope4D RopePositionInputPositionùëßùëß!FeaturePositionFrozenTrainableAttn. Pool + Projùëß"Quantization (Optional)Discrete TokenContinuous latentÃÉùëß!orùëß!Preprint

Figure 3: 3D tokenization pipeline. We extend Trellis-SLAT (Xiang et al., 2024) for multimodal unification
through two modifications: directly tokenizing raw RGB patches from multiview renderings (as opposed to
using DINOv2 features), and aggregating each voxel‚Äôs features from its nearest viewpoint (as opposed to aver-
aging across all views). Combined with Gaussian decoding, this approach integrates 3D assets into our unified
token space alongside images and videos.

generate Gaussian splatting parameters for efficient rendering:

DGS : {(zi, pi)}L

i=1 ‚Üí {{(ok

i , ck

i , sk

i , Œ±k

i , rk

i )}K

k=1}L

i=1

(3)

where each location generates K Gaussians with parameters: position offset o, color c, scale s,
opacity Œ±, and rotation r. Following Xiang et al. (2024), we constrain Gaussian positions to remain
near their source voxels using xk

i ), ensuring local feature coherence.

i = pi + tanh(ok

3.3 TRAINING OBJECTIVES

We jointly optimize for reconstruction fidelity and semantic understanding through an adversarial-
free training objective:

L = ŒªrecLrec + ŒªsemLsem + ŒªKLLKL,

(4)

where LKL is the KL regularization term applied to the projected reconstruction latents zr, with
Œªrec, Œªsem and ŒªKL balancing components. Notably, we achieve state-of-the-art reconstruction quality
without adversarial training, which has been observed to be unstable when scaling (Wu et al., 2025a)
and incompatible with our sparse 3D representations.

Reconstruction Loss. While GANs (Goodfellow et al., 2014) are standard for visual tokenizers,
we found them unsuitable for our transformer architecture. Figure 4(a) shows the discriminator
rapidly dominates the generator, causing mode collapse and degraded reconstruction quality. To
develop an alternative, we analyzed the reconstruction error by decomposing rFID into mean and
covariance components (Figure 4(b)). The covariance component ‚Äì capturing second-order statistics
like texture and style ‚Äì dominates at ‚âà 86.6%, while mean features contribute only 13.4%.

This insight motivated adopting Gram matrix loss (Gatys et al., 2016), which directly optimizes
feature covariance without adversarial training:

LGram(x, ÀÜx) =

(cid:88)

l

‚à•G(Œ¶l(x)) ‚àí G(Œ¶l( ÀÜx))‚à•2

F ,

(5)

where G(F ) = F F ‚ä§ is the Gram matrix for feature map F from layer l of network Œ¶. As shown in
Figure 4(c), this achieves superior and stable reconstruction throughout training.

For images, we combine four complementary loss components:

LI

rec = Œª1L1 + ŒªLPIPSLLPIPS + ŒªGRAMLGRAM + ŒªCLIPLCLIP,

(6)

where L1 = ‚à•x ‚àí ÀÜx‚à•1 provides pixel supervision, LLPIPS (Zhang et al., 2018) measures perceptual
similarity, LGRAM captures texture, and LCLIP enforces semantic consistency. For video and 3D
assets, we use LV/3D

rec = L1 for efficiency, relying on cross-modal transfer from images for details.

6

3D AssetMulti-view RenderingsOccupancy GridFind Nearest Patch and Fill RGB ValuesUnderstandingGenerationFeatsPos.‚Ñ∞ùíüFeatsPos.Gaussian Proj Head Preprint

(a) GAN training instability

(b) Decomposition of rFID.

(c) Gram loss efficiency

Figure 4: Adversarial-free training with Gram loss achieves stable, high-fidelity reconstruction. (a) GAN
training fails in our setting: the discriminator overpowers the generator, causing diverging logits and degraded
rFID. (b) Decomposing rFID reveals ‚âà 86.6% of error stems from covariance (texture/style) vs. ‚âà 13.4% from
mean components. (c) Gram loss directly optimizes second-order statistics (i.e., feature covariance) without
adversarial training, achieving superior and stable rFID throughout training.

Figure 5: Progressive training curriculum of AToken. Our model starts from SigLIP2 image understanding
and progressively adds: (1) image reconstruction, (2) video capabilities with temporal modeling, (3) 3D under-
standing with expanded resolutions, and optionally (4) discrete tokenization via FSQ. Each box shows the new
capabilities introduced at that stage, along with supported resolutions, patch sizes, and sampling strategies.

Semantic Loss. We align visual representations zs with text embeddings through modality-
specific objectives. For images, we distill knowledge from the frozen SigLIP2 vision encoder
(Tschannen et al., 2025) by minimizing the KL divergence between temperature-scaled vision-text
similarity distributions:

sem = KL (cid:0)softmax(œÑ ‚àí1steacher) ‚à• softmax(œÑ ‚àí1sstudent)(cid:1) ,
LI
where steacher and sstudent are vision-text similarity scores from frozen SigLIP2 and our model re-
spectively, both paired with the same frozen text encoder, and œÑ is the temperature parameter. For
videos and 3D, we directly optimize alignment using the sigmoid loss from SigLIP (Zhai et al.,
2023), which proves more stable for the smaller batch sizes typical in these domains. This dual
strategy preserves pretrained image semantics while enabling efficient learning for new modalities.

(7)

3.4 TRAINING STRATEGY

Our training employs a four-stage progressive curriculum (Figure 5) that builds from image foun-
dations to video dynamics to 3D geometry, with optional discrete quantization. Starting from the
pretrained SigLIP2 encoder (Tschannen et al., 2025), we gradually introduce more complex objec-
tives and modalities while maintaining semantic understanding across all stages.

We implement this curriculum through round-robin sampling of modalities and tasks, using gradi-
ent accumulation to balance image-text distillation with other objectives (reconstruction, video-text
alignment, 3D-text alignment) across all stages. This ensures semantic alignment is preserved even
as reconstruction capabilities expand. Our sparse transformer architecture facilitates this multi-
modal training by separating features and positions, allowing each modality to be processed at its
natural resolution without padding or packing.

Stage 1: Image Foundation. Starting from pretrained SigLIP2, we establish core visual represen-
tations by adding image reconstruction capabilities. We process images using 4√ó16√ó16 space-time
patches with temporal padding for consistency, employing 32 latent dimensions following (Yao &

7

rfid scoreLogit ValueSmoothed Real vs. Fake Logits During TrainingrFid Score During TrainingTraining StepProportion of TotalTraining StepsrFIDImage Und(SigLIP2)Stage 1: Image RecStage 2:Video RecVideo UndStage 3: 3D Rec3D UndStage 4: Quantization(FSQ)SigLIP2-so400m-naflexPatch size: 16x1664-512 px imagesPatch size: 4x16x1664-1024 px images64-512 px videosVideo Und: Up to 64@1FPSVideo Rec: Up to 32@1‚Äì12 stride64-2048 px images64-1024 px videos3D Und/Rec: 64x 64 x 64 voxels8x6D codebooks2-bit per dim4096 vocab eachPreprint

Figure 6: Overview of the video encoding and decoding process. During encoding, we use KV-caching
across temporal tiles to eliminate redundant computation while maintaining temporal coherence, providing
significant efficiency gains over overlapping tile methods.

Table 2: Training curriculum configuration. Resolution limits for each modality and task sampling ratios
across the four training stages. Superscripts denote reconstruction (r) and understanding (u) tasks.

Training Stage

Image Res.

Video Res.

3D Size

Stage 1: Image Foundation
Stage 2: Video Dynamics
Stage 3: 3D Geometry
Stage 4: Discrete Tokenization

[64 ‚Üí 512]
[64 ‚Üí 1024]
[64 ‚Üí 2048]
[64 ‚Üí 2048]

-
[64 ‚Üí 512]
[64 ‚Üí 1024]
[64 ‚Üí 1024]

-
-
[64, 64, 64]
[64, 64, 64]

Task Sampling Ratios

Ir

Vu

Vr

3Du

3Dr

#Steps

-

200k
-
100%
200k
22.2% 11.1% 66.6%
22.2% 11.1% 44.4% 11.1% 11.1%
50k
22.2% 11.1% 44.4% 11.1% 11.1% 100k

-
-

-
-

Wang, 2025). Training uses variable resolution sampling from 64 to 512 pixels, with L1 loss com-
puted at native resolution while perceptual losses (LLPIPS, LCLIP, LGram) use 224 √ó 224 interpolation
to match their pretrained features.

Stage 2: Video Dynamics. We extend to temporal sequences, expanding latent dimensions from
32 to 48 to accommodate motion complexity (Seawead et al., 2025). Resolution capabilities in-
crease to 1024 for images and 512 for videos. We employ temporal tiling (16-32 frames ‚Üí 4-8
latent frames) with adaptive sampling: stride 1-3 for temporal consistency or 4-12 for diversity in
reconstruction, 1 FPS up to 64 frames for understanding. Our KV-caching mechanism (Figure 6)
eliminates redundant computation across tiles while maintaining temporal coherence.

Stage 3: 3D Geometry. We incorporate 3D assets as active voxels in 643 grids, using Gaussian
splatting for reconstruction and attention pooling for understanding. Resolution further increases
to 2048 for images and 1024 for videos. Joint optimization across all three modalities prevents
catastrophic forgetting while leveraging cross-modal learning. The geometric semantics from 3D
and the temporal dynamics from video enhance image reconstruction quality.

Stage 4: Discrete Tokenization. Optionally, we add FSQ quantization (Mentzer et al., 2023) for
discrete generation tasks. The 48-dimensional latents are partitioned into 8 groups of 6 dimensions,
each quantized to 4 levels, yielding 8 discrete tokens from 4096-entry codebooks. We finetune the
entire encoder and decoder to adapt all modalities to discrete tokens, enabling compatibility with
discrete generative models across all visual domains.

3.5

IMPLEMENTATION DETAILS

Our encoder and decoder each contain 27 transformer blocks with hidden dimension d = 1152 and
16 attention heads. The encoder is initialized from SigLIP-SO400M-patch16-naflex (Tschannen
et al., 2025), while the decoder is trained from scratch.

We optimize using AdamW with Œ≤1 = 0.9, Œ≤2 = 0.95, and weight decay 0.1. The learning rate
follows linear warmup for 2,000 steps to Œ∑max = 3 √ó 10‚àí4, then cosine annealing to Œ∑min =
3 √ó 10‚àí5. Given the pretrained encoder, we apply a reduced learning rate Œ∑encoder = 0.1 √ó Œ∑base and
use exponential moving average with decay rate Œ≥ = 0.9999.

Training utilizes 256 H100 GPUs with adaptive global batch sizes optimized for each task‚Äôs mem-
ory requirements. Image understanding maintains 8,192 samples throughout all stages, while recon-
struction tasks scale with complexity: image reconstruction uses 1,024-4,096, video reconstruction

8

Input videoTemporal tilesTemporal latent tilesEncoded latent(a) Video Encoding (Training)(b) Video Decoding (Inference)Reconstructed videoTemporal tilesTemporal latent tiles with KV-cacheDecoded latentwith KV-cache2020202051616161616444445111155Preprint

Table 3: Performance comparison of visual tokenizers across modalities. We evaluate on ImageNet for
image reconstruction and zero-shot classification, TokenBench for video reconstruction with MSR-VTT for
zero-shot retrieval, and Toys4k for 3D reconstruction and classification. Methods are grouped by capability:
reconstruction-only, understanding-only, and unified approaches. Discrete tokenizers are indicated with gray
shading. ‚Ä† OmniTokenizer does not work well on high-resolution videos where tiling is needed.

Comp.
Ratio

Latent
Channels

Token
Type

Image

Video

3D

PSNR‚Üë rFID‚Üì Acc.‚Üë PSNR‚Üë rFVD‚Üì R@1‚Üë PSNR‚Üë LPIPS‚Üì Acc.‚Üë

Method

Reconstruction Only

(1, 8, 8)
SD-VAE
(1, 8, 8)
FLUX.1 [dev]
Cosmos-0.1-CI8√ó8
(1, 8, 8)
(1, 8, 8)
Qwen-Image
(1, 16, 16)
VA-VAE
GigaTok-XL-XXL
(1, 16, 16)
Cosmos-0.1-CV8√ó8 (4, 8, 8)
OmniTokenizer‚Ä†
(4, 8, 8)
(4, 8, 8)
Hunyuan
(4, 8, 8)
Wan2.1
(4, 16, 16)
Wan2.2
OmniTokenizer‚Ä†
(4, 8, 8)
Cosmos-0.1-DV8√ó8 (4, 8, 8)
Trellis-SLAT

-

Understanding Only
VideoPrism-g
SigLIP2-So/16
PEcoreL

(1, 18, 18)
(1, 16, 16)
(1, 14, 14)

Reconstruction & Understanding

4
16
16
16
32
8
16
8
16
16
48
8
6
8

-
-
-

SeTok
VILA-U
UniTok

ATOKEN-So/D
ATOKEN-So/C

-
(1, 16, 16)
(1, 16, 16)

(4, 16, 16)
(4, 16, 16)

4096
16
64

48
48

VAE
VAE
AE
VAE
VAE
VQ
AE
VAE
VAE
VAE
VAE
VQ
FSQ
VAE

-
-
-

AE
RQ
MCQ

FSQ
VAE

26.26
32.86
32.25
32.18
27.70
22.42
30.11
26.74
33.32
31.34
31.25
24.69
26.34
-

-
-
-

-
22.24
25.34

27.00
29.72

0.61
0.18
1.03
1.46
0.28
0.80
7.55
1.02
0.67
0.94
0.75
1.41
7.86
-

-
-
-

2.07
4.23
0.36

0.38
0.21

-
-
-
-
-
-
-
-
-
-
-
-
-
-

-
83.4
83.5

75.4
78.0
78.6

82.2
82.2

-
-
-
-
-
-
34.33
19.39
36.37
36.11
36.39
19.89
31.42
-

-
-
-

-
-
-

-
-
-
-
-
-
8.34
173.48
3.78
3.21
3.19
202.46
25.94
-

-
-
-
-
-
-
-
-
-
-
-
-
-
-

-
-
-

-
-
-

52.7
41.9
50.3

-
-
-

-
-
-
-
-
-
-
-
-
-
-
-
-
26.97

-
-
-

-
-
-

-
-
-
-
-
-
-
-
-
-
-
-
-
0.054

-
-
-

-
-
-

-
-
-
-
-
-
-
-
-
-
-
-
-
-

-
-
-

-
-
-

33.12
36.07

22.16
3.01

40.3
40.2

28.17
28.28

0.063
0.062

91.3
90.9

uses 512-1024, and 3D reconstruction uses 256-512. The four-stage curriculum trains for 200k,
200k, 50k, and 100k iterations, respectively, with each stage initialized from the previous check-
point, requiring a total of 138k GPU hours across all stages (approximately 22 days with 256 GPUs).
Throughout training, we maintain fixed loss coefficients: Œªrec = 0.2, Œªsem = 1.0, and ŒªKL = 10‚àí8.
Within reconstruction (Eq. 6), we set Œª1 = 1.0, ŒªLPIPS = 10.0, ŒªGRAM = 103, ŒªCLIP = 1.0, and
œÑ = 2.0. We normalize reconstruction losses over patches rather than summing (Esser et al., 2020),
providing stable gradients across resolutions.

Training data follows our progressive curriculum: DFN (Fang et al., 2023), Open Images
(Kuznetsova et al., 2020), and internal datasets for images; WebVid (Bain et al., 2021) and TextVR
(Wu et al., 2025c) for video understanding with Panda70M (Chen et al., 2024b) for reconstruction;
Objaverse (Deitke et al., 2023) with Cap3D (Luo et al., 2024a) annotations for 3D. Datasets are
sampled proportionally to their size, with task ratios detailed in Table 2.

4 MAIN RESULTS

We evaluate ATOKEN as the first visual tokenizer to achieve both reconstruction and understand-
ing across images, videos, and 3D assets. This section presents unified comparisons (Section 4.1)
followed by per-modality analysis (Sections 4.2-4.4) and ablations (Section 4.5).

4.1 UNIFIED TOKENIZER COMPARISONS

Table 3 presents a comparison of visual tokenizers across modalities. We evaluate on standardized
benchmarks: ImageNet (Deng et al., 2009) at 256√ó256 (reconstruction: PSNR, rFID; understand-
ing: zero-shot accuracy), TokenBench (Agarwal et al., 2025) at 720p and MSR-VTT (Xu et al.,
2016) for video (reconstruction: PSNR, rFVD; understanding:
text-to-video R@1), and Toys4k
(Stojanov et al., 2021a) for 3D (reconstruction: PSNR, LPIPS; understanding: zero-shot accuracy).

9

Preprint

Table 4: Image reconstruction comparison on ImageNet and COCO. We evaluate all methods using a
unified protocol with official implementations to ensure fair comparison. All images are resized and center-
cropped to 256√ó256, with metrics computed using identical scripts. Note that our reproduced results may differ
from original papers due to standardized evaluation settings, but provide consistent cross-model comparison.

Method

Continuous Latent

SD-VAE
SD3-VAE
FLUX.1 [dev]
Qwen-Image
Cosmos-0.1-CI8√ó8
Cosmos-0.1-CI16√ó16
VAVAE
OmniTokenizer
Hunyuan
Wan2.1
Wan2.2

ATOKEN-So/C

Stage 1
Stage 2
Stage 3

Discrete Latent

Cosmos-0.1-DI8√ó8
GigaTok-B-L
GigaTok-XL-XXL
Vila-U
UniTok
OmniTokenizer

ATOKEN-So/D

Comp.
Ratio

Latent
Size

Token
Type

ImageNet

COCO

PSNR‚Üë

SSIM‚Üë LPIPS‚Üì

rFID‚Üì

PSNR‚Üë

SSIM‚Üë LPIPS‚Üì

rFID‚Üì

(1, 8, 8)
(1, 8, 8)
(1, 8, 8)
(1, 8, 8)
(1, 8, 8)
(1, 16, 16)
(1, 16, 16)
(4, 8, 8)
(4, 8, 8)
(4, 8, 8)
(4, 16, 16)

(1, 16, 16)
(4, 16, 16)
(4, 16, 16)

(1, 8, 8)
(1, 16, 16)
(1, 16, 16)
(1, 16, 16)
(1, 16, 16)
(4, 8, 8)

(4, 16, 16)

4
16
16
16
16
16
32
8
16
16
48

32
48
48

6
8
8
16
64
8

48

VAE
VAE
VAE
VAE
AE
AE
VAE
VAE
VAE
VAE
VAE

VAE
VAE
VAE

FSQ
VQ
VQ
RQ
MCQ
VQ

FSQ

26.26
31.29
32.86
32.18
32.25
25.07
27.70
26.74
33.32
31.34
31.25

28.77
29.55
29.72

25.87
21.87
22.42
22.24
25.34
24.69

27.14

0.745
0.886
0.917
0.899
0.902
0.700
0.798
0.824
0.916
0.886
0.878

0.814
0.845
0.848

0.750
0.591
0.613
0.612
0.742
0.771

0.801

0.133
0.059
0.044
0.053
0.064
0.167
0.096
0.101
0.053
0.058
0.057

0.099
0.087
0.085

0.155
0.200
0.189
0.228
0.132
0.138

0.119

0.606
0.201
0.176
1.459
1.031
0.959
0.279
1.023
0.670
0.945
0.749

0.258
0.246
0.209

0.867
0.507
0.795
4.231
0.362
1.411

0.379

25.99
31.18
32.73
32.01
32.08
24.74
27.50
26.44
33.25
31.19
31.10

28.66
29.49
29.67

25.54
21.42
22.03
21.89
24.95
24.31

27.00

0.759
0.894
0.923
0.908
0.909
0.711
0.811
0.833
0.924
0.895
0.888

0.829
0.858
0.861

0.760
0.596
0.620
0.620
0.750
0.779

0.815

0.130
0.056
0.041
0.050
0.061
0.165
0.093
0.099
0.050
0.055
0.054

0.096
0.083
0.081

0.153
0.202
0.191
0.227
0.131
0.137

0.115

4.142
1.671
1.343
4.618
3.844
5.063
2.709
4.687
2.597
3.449
3.279

2.336
2.180
2.026

5.016
5.565
5.757
10.997
3.918
6.292

3.270

The results reveal three distinct categories of approaches, each with fundamental limitations.
Reconstruction-only tokenizers excel at generation but cannot extract semantic features: SD-
VAE (Rombach et al., 2022), FLUX.1 (Labs et al., 2025), VA-VAE (Yao & Wang, 2025), and
Qwen-Image (Wu et al., 2025a) for images; Hunyuan (Kong et al., 2024) and WAN (Wan et al.,
2025) for video; Trellis-SLAT (Xiang et al., 2024) for 3D. Understanding-only encoders provide
rich semantics but cannot reconstruct visual content: SigLIP2 (Tschannen et al., 2025), Video-
Prism (Zhao et al., 2024), and PEcore (Bolya et al., 2025). Recent unified attempts combine both
capabilities but remain limited to images: SeTok (Wu et al., 2024b), VILA-U (Wu et al., 2024c),
and UniTok (Ma et al., 2025a).

ATOKEN-So/C breaks these boundaries as the first tokenizer to unify all three capabilities. On im-
ages, we achieve 0.21 rFID with 82.2% zero-shot ImageNet accuracy, substantially outperforming
UniTok‚Äôs 0.36 rFID and 78.6% accuracy. More importantly, we extend this unified capability to
video (3.01 rFVD, 40.2% R@1) and 3D (28.28 PSNR, 90.9% accuracy), comparable or even sur-
passing specialized methods like Wan2.2 and Trellis-SLAT on Video and 3D reconstruction. Our
discrete variant (ATOKEN-So/D) maintains competitive performance, pioneering discrete tokeniza-
tion across all modalities.

4.2

IMAGE TOKENIZATION

We evaluate ATOKEN‚Äôs image capabilities against specialized tokenizers through reconstruction
quality (Table 4) and semantic understanding (Table 5) benchmarks.

Reconstruction Performance. Table 4 presents our comprehensive evaluation, where we re-
evaluated all baseline methods using a unified protocol with official implementations to ensure
fair comparison. Under this standardized evaluation protocol, we observe that multimodal train-
ing enhances rather than compromises image reconstruction. ATOKEN-So/C achieves 0.209 rFID at
16√ó16 compression, with progressive improvement across training stages: 0.258 (Stage 1)‚Üí0.246
(Stage 2)‚Üí0.209 (Stage 3), a 19% gain through multimodal expansion.

This improvement is particularly notable given three fundamental challenges in the field. First, the
compression-dimension trade-off severely constrains 16√ó16 models: VAVAE (Yao & Wang, 2025)
requires 32-dimensional latents to achieve 0.279 rFID, while Cosmos-CI16√ó16 with 16 dimensions
degrades to 0.959 rFID. Second, transformer architectures consistently underperform convolutional
architectures (OmniTokenizer (Wang et al., 2024b) 26.74 PSNR vs. Hunyuan (Kong et al., 2024)
33.32 PSNR), explaining why most reconstruction tokenizers avoid transformers. Third, discrete

10

Preprint

Table 5: Image understanding comparison with semantic encoders. We evaluate zero-shot classification on
ImageNet, ImageNet-v2, and cross-modal retrieval on COCO and Flickr30k. ATOKEN maintains competitive
performance across all stages despite joint training on multiple modalities and tasks.

Res.

Seq. Model

224

196

256

256

384

576

512

1024

CLIP
MetaCLIP
EVA-CLIP
DFN

SigLIP
SigLIP 2

ATOKEN-So/C

Stage 1
Stage 2
Stage 3

ATOKEN-So/D

SigLIP 2

ATOKEN-So/C

Stage 1
Stage 2
Stage 3

ATOKEN-So/D

SigLIP 2

ATOKEN-So/C

Stage 1
Stage 2
Stage 3

ATOKEN-So/D

ImageNet-1k

COCO

Flickr

val

68.3
72.4
74.7
76.2

80.8
83.4

82.7
82.3
82.2
82.2

84.1

83.4
82.9
82.9
82.8

84.3

83.5
83.1
82.9
82.9

v2

61.9
65.1
67.0
68.2

74.1
77.8

76.7
76.4
76.1
76.2

78.4

77.6
77.1
76.8
76.6

79.1

77.8
77.3
77.2
77.0

T‚ÜíI

I‚ÜíT T‚ÜíI

I‚ÜíT

33.1
48.9
42.2
51.9

49.4
55.4

54.1
53.8
53.7
53.8

56.0

54.8
54.7
54.6
54.4

56.0

54.7
54.7
54.7
54.7

52.4
‚Äì
58.7
‚Äì

68.6
71.5

70.4
70.6
70.5
70.1

71.2

70.4
71.1
71.3
70.9

71.3

71.1
71.3
71.1
71.2

62.1
77.1
71.2
77.3

80.0
84.4

81.3
80.7
80.5
80.9

85.3

81.7
81.9
81.9
81.9

85.5

82.1
82.2
82.3
82.3

81.9
‚Äì
85.7
‚Äì

92.1
94.2

93.1
93.0
93.2
93.5

95.9

93.8
93.9
93.5
93.5

95.4

94.1
93.6
93.6
93.5

tokenizers struggle with generalization ‚Äì UniTok (Ma et al., 2025a) degrades from 0.362 rFID on
ImageNet to 3.918 on COCO, while GigaTok (Xiong et al., 2025) exhibits even larger gaps.

Our approach addresses all three challenges: achieving strong performance with 48-dimensional
latents at 16√ó16 compression, demonstrating transformer viability through adversarial-free training,
and maintaining consistent quality across datasets (0.209 rFID on ImageNet, 2.026 rFID on COCO).
These results suggest temporal dynamics from video and geometric understanding from 3D provide
complementary signals for image reconstruction.

Semantic Understanding. Table 5 evaluates zero-shot classification and retrieval against leading
vision encoders. While understanding-only models like CLIP (Radford et al., 2021) and its vari-
ants (Xu et al., 2023; Sun et al., 2023; Fang et al., 2023) optimize purely for semantic alignment,
ATOKEN need to balance understanding with reconstruction across three modalities.

Despite these constraints, ATOKEN achieves 82.2% ImageNet accuracy ‚Äì within 1.2% of
understanding-only SigLIP2 (Tschannen et al., 2025) (83.4%). This narrows the gap compared
to previous unified attempts like UniTok (78.6%) and VILA-U (78.0%), while uniquely extending
unified capabilities to video and 3D. Across our progressive training stages, accuracy remains sta-
ble (82.7% ‚Üí 82.3% ‚Üí 82.2%), with only 0.5% degradation as modalities are added. Discrete
quantization also preserves full semantic performance, achieving 82.2% accuracy.

4.3 VIDEO TOKENIZATION

We evaluate ATOKEN‚Äôs video capabilities through reconstruction quality and semantic understand-
ing benchmarks, demonstrating competitive performance while uniquely supporting both continuous
and discrete representations across multiple modalities.

Reconstruction Performance. We evaluate video reconstruction on DAVIS (Pont-Tuset et al.,
2017) (1080p, 50 videos) and TokenBench (Agarwal et al., 2025) (720p, 471 videos), reporting
PSNR and SSIM for pixel quality, LPIPS for perceptual similarity, and rFVD for temporal con-
sistency. All baselines were re-evaluated using official implementations with consistent protocols
and spatial tiling for memory management. ATOKEN employs temporal tiling with KV-caching,
leveraging its native 2048√ó2048 resolution support.

11

Preprint

Table 6: Video reconstruction comparison on high-resolution benchmarks. We evaluate quality on DAVIS
at 1080p and TokenBench at 720p. All methods are re-evaluated using official implementations with consistent
protocols for fair comparison. ATOKEN achieves competitive performance with specialized video-only tok-
enizers while uniquely supporting both continuous and discrete representations across modalities.

Tokenizer

Continuous Latent

Comp.
Ratio

Latent
Size

Token
Type

DAVIS

TokenBench

PSNR‚Üë

SSIM‚Üë LPIPS‚Üì

rFVD‚Üì PSNR‚Üë

SSIM‚Üë LPIPS‚Üì

rFVD‚Üì

Cosmos-0.1-CV4√ó8√ó8
OmniTokenizer
Hunyuan
Wan2.1
Wan2.2

(4, 8, 8)
(4, 8, 8)
(4, 8, 8)
(4, 8, 8)
(4, 16, 16)

ATOKEN-So/C

Stage 2
Stage 3

Discrete Latent

OmniTokenizer
Cosmos-0.1-DV4√ó8√ó8

ATOKEN-So/D

(4, 16, 16)
(4, 16, 16)

(4, 8, 8)
(4, 8, 8)

(4, 16, 16)

16
8
16
16
48

48
48

8
6

48

AE
VAE
VAE
VAE
VAE

VAE
VAE

VQ
FSQ

FSQ

32.25
21.06
32.33
33.50
33.06

32.29
33.11

20.62
27.26

29.75

0.894
0.800
0.907
0.884
0.907

0.902
0.907

0.770
0.798

0.846

0.219
0.315
0.194
0.164
0.184

0.196
0.189

0.346
0.310

0.288

19.15
206.34
22.94
17.75
12.65

13.50
10.76

240.20
110.33

41.42

34.33
19.39
36.37
36.11
36.39

35.63
36.07

19.89
31.20

33.12

0.924
0.782
0.944
0.940
0.942

0.937
0.940

0.787
0.892

0.913

0.155
0.275
0.129
0.128
0.126

0.139
0.135

0.293
0.190

0.193

8.34
173.48
3.78
3.21
3.19

3.63
3.01

202.46
25.94

22.16

Table 7: Zero-shot video-text retrieval on MSRVTT and MSVD. We compare ATOKEN against
understanding-focused encoders on standard video retrieval benchmarks. Despite optimizing for both recon-
struction and understanding across three modalities, ATOKEN maintains reasonable retrieval performance.

MSRVTT (1K-A)

MSVD

Methods

Res.

Text ‚Üí Video

Video ‚Üí Text

Text ‚Üí Video

Video ‚Üí Text

R@1

R@5

R@10

R@1

R@5

R@10

R@1

R@5

R@10

R@1

R@5

R@10

CLIP-ViT-B/32
SigLIP2-So400m
VideoPrism-g
PE-Core-B16
PE-Core-L14

ATOKEN-So/C‚Äì224

Stage 1
Stage 2
Stage 3

ATOKEN-So/D

224
256
288
224
336

224
224
224
224

31.2
41.9
52.7
45.8
49.1

40.8
40.1
40.2
40.3

53.7
66.3
77.2
70.1
73.3

65.3
64.9
64.9
65.0

63.3
75.7
-
78.1
81.6

75.2
75.2
75.2
74.6

26.4
32.4
51.7
45.5
50.9

31.0
30.9
30.5
30.3

49.9
55.4
75.2
70.9
74.4

55.0
53.7
53.1
51.8

61.7
65.9
-
80.0
82.7

63.7
64.0
63.2
61.7

36.4
55.5
-
48.7
54.4

53.9
53.4
53.5
53.8

63.3
81.2
-
75.5
81.2

79.9
79.6
79.5
79.7

73.1
87.8
-
84.1
88.4

87.3
87.1
87.1
87.2

57.8
72.7
-
79.1
82.5

72.4
71.6
72.4
71.5

84.1
91.7
-
96.7
98.2

93.0
91.9
91.6
91.8

90.7
96.1
-
98.8
99.4

95.4
95.5
95.4
95.2

As shown in Table 6, ATOKEN-So/C achieves 33.11 PSNR on DAVIS and 36.07 PSNR on Token-
Bench, approaching specialized video-only models (Wan2.1 (Wan et al., 2025): 33.50 and 36.11,
Hunyuan (Kong et al., 2024): 32.33 and 36.37). Notably, we demonstrate that transformers can
match CNN performance when properly designed ‚Äì our method dramatically outperforms OmniTo-
kenizer‚Äôs transformer baseline (21.06 vs 33.11 PSNR on DAVIS) while adding native resolution sup-
port. Furthermore, our progressive training reveals cross-modal benefits: incorporating 3D in Stage
3 improves video reconstruction from 35.63 to 36.07 PSNR on TokenBench, indicating that geo-
metric understanding may enhance temporal modeling. For discrete tokenization, ATOKEN-So/D
pioneers multimodal video support, achieving 29.75 PSNR on DAVIS ‚Äì surpassing Cosmos-0.1-
DV (27.26) and dramatically outperforming OmniTokenizer (20.62), while maintaining reasonable
perceptual quality (0.288 LPIPS) for downstream tasks.

Semantic Understanding. Table 7 evaluates zero-shot video-text retrieval on MSRVTT (Xu et al.,
2016) and MSVD (Chen & Dolan, 2011). Following standard protocols (Wang et al., 2022b; Luo
et al., 2021), we use frame embedding averaging with zero-padding. ATOKEN achieves 40.2% R@1
on MSRVTT and 53.5% on MSVD, maintaining reasonable semantic alignment despite optimizing
primarily for reconstruction across three modalities. We note that alternative pooling strategies
without frame averaging yielded lower performance, likely due to the limited video-text pairs in
our training data compared to dedicated video understanding models. While understanding-only
models trained on large-scale video-text data achieve higher scores, our results validate that unified
tokenization successfully balances reconstruction quality with semantic understanding.

4.4

3D TOKENIZATION.

We evaluate ATOKEN‚Äôs 3D capabilities on Toys4k (Stojanov et al., 2021b) for reconstruction and
semantic understanding. For reconstruction, ATOKEN-So/C achieves 28.28 PSNR and 0.062 LPIPS
(Table 8), surpassing the specialized Trellis-SLAT (Xiang et al., 2024) baseline (26.97 PSNR, 0.054

12

Preprint

Figure 7: Architectural scaling comparison: Base vs. So400m models. (a) ImageNet rFID during Stage
1 training. (b) ImageNet rFID across training stages. (c) ImageNet zero-shot classification accuracy in Stage
1. (d) Video PSNR on DAVIS in Stages 2 and 3. The So400m model maintains or improves performance
across all stages, while the Base model shows significant degradation when extending beyond single-modality
training, indicating that sufficient model capacity is critical for successful multimodal visual tokenization.

Table 8: 3D reconstruction comparison on Toys4k. We average metrics across rendered multi-view images.
ATOKEN achieves comparable performance to specialized Trellis-SLAT despite jointly optimizing for three
modalities, demonstrating unified training maintains strong 3D capabilities.

Method

PSNR‚Üë

SSIM‚Üë LPIPS‚Üì

Specialized 3D Tokenizer

Trellis-SLAT

26.97

0.943

0.054

Our Unified Tokenizer (ATOKEN)

ATOKEN -So/C
ATOKEN -So/D

28.28
28.17

0.951
0.951

0.062
0.063

LPIPS) despite jointly training across three modalities. This demonstrates that our unified 4D rep-
resentation effectively captures geometric structure without requiring dedicated 3D architectures.

For semantic understanding, ATOKEN-So/C achieves 90.9% zero-shot classification accuracy on
Toys4k, validating that our approach maintains strong semantic representations for 3D objects along-
side reconstruction capabilities. Combined with our image and video results, this confirms that all
three modalities can coexist within a single tokenizer without significant trade-offs.

4.5 ABLATION STUDY

Scaling Analysis. To investigate the scaling property of the visual tokenizer, we compare our
So400m model with a smaller Base variant following identical training procedures. The Base model
initializes from SigLIP-Base-patch16-naflex (Tschannen et al., 2025), comprising 12 transformer
blocks with hidden dimension d = 768 and 12 attention heads for both encoder and decoder, yielding
approximately 192M parameters compared to So400m‚Äôs 800M.

As shown in Figure 7, both models achieve reasonable single-modal performance in Stage 1, with
So400m outperforming Base (0.258 vs 0.323 rFID, 82.7% vs 77.2% accuracy). However, the Base
model suffers severe degradation when expanding to videos, with ImageNet rFID degrading 49%
(0.323‚Üí0.483) and video PSNR declining across stages. In contrast, So400m improves continu-
ously ‚Äì ImageNet rFID enhances 19% (0.258‚Üí0.209) while video PSNR rises from 32.51 to 33.11.
This scaling analysis reveals that multimodal tokenization has a capacity requirement: small models
suffer from interference while large models benefit from cross-modal learning.

Representation Structure Analysis. Figure 8 visualizes learned representations through T-SNE
projections across training stages. Dense features (a-c) show clear semantic clustering with distinct
ImageNet class separation. However, projection to 48-dimensional latents (d-e) results in more
intermixed distributions, likely due to KL regularization without post-projection alignment loss.

Despite this apparent mixing in T-SNE visualizations, the model maintains strong reconstruction
and understanding performance, suggesting that semantic information may be encoded in ways not
captured by 2D projections. This raises an interesting question: whether explicit semantic cluster-

13

(a)(b)(c)(d)rFID (Lower is Better)Training Step0.30.40.50.60.70.80.91.050k100k150k200kBaseSo400mImageNet rFID in Stage 1Training Step75.5%76.5%Zero-Shot Accuracy@177.5%78.5%80.5%81.5%82.5%83.5%40k80k120k160k200kSo400mBasePSNR (Higher is Better)Training Stage29.7031.0331.1630.9629.4430.5032.0832.2932.5133.11So400mBase29.030.031.032.033.050k100k150k200k250krFID (Lower is Better)0.3280.4830.4470.2580.2460.209So400mBase0.150.200.250.300.350.400.450.50Stage 1Stage 2Stage 3Training StepImageNet rFID across stagesImageNet zero-shot classification in Stage 1Video PSNR (Davis) in stage 2 and 3Preprint

Figure 8: Learned representations across training stages. T-SNE visualizations of ImageNet class embed-
dings (colors indicate different classes). (a) Stage 1: image-only training. (b) Stage 2: with video. (c) Stage 3:
dense features before projection. (d) Stage 3: projected 48-dim latents. (e) Stage 4: before FSQ quantization.
Dense features (a-c) show clear semantic clustering, while dimensional reduction (d-e) leads to more mixed
class distributions, suggesting a trade-off between compression and semantic separability.

ing in low-dimensional spaces ‚Äì as emphasized by methods like VAVAE (Yao & Wang, 2025) ‚Äì
is necessary for strong performance, or whether larger models can effectively leverage seemingly
intermixed representations. Our results suggest the latter, though we leave detailed investigation of
semantic preservation through aggressive dimensionality reduction for future work.

Reconstruction Visualization. Figures 9-11 provide qualitative comparisons of reconstruction
quality across all three modalities. For images (Figure 9), ATOKEN operates at a higher compres-
sion ratio (16√ó) than most baselines yet achieves superior visual fidelity, particularly in preserving
high-frequency details such as text clarity, fine textures, and complex patterns. The comparison
reveals that methods optimized for lower compression ratios (e.g., SD-VAE and OmniTok at 8√ó)
struggle with text legibility and texture preservation, while ATOKEN maintains sharp details. For
video reconstruction (Figure 10), ATOKEN demonstrates temporal consistency comparable to spe-
cialized video tokenizers like Wan2.2, with both continuous and discrete variants preserving motion
smoothness across 720p sequences. The 3D reconstruction results (Figure 11) highlight ATOKEN‚Äôs
advantage in color consistency. While Trellis-SLAT exhibits color shifts and artifacts, our unified
training across modalities transfers color understanding from images and videos to improve 3D re-
construction.

5 DOWNSTREAM RESULTS

Having established ATOKEN‚Äôs unified tokenization capabilities across modalities, we evaluate its
effectiveness in diverse downstream applications. We assess both understanding tasks through mul-
timodal LLMs (Section 5.1) and generation tasks across images, videos, and 3D assets (Sections 5.2‚Äì
5.5). These experiments demonstrate that a single unified tokenizer can serve as the foundation for
multimodal AI systems without compromising task-specific performance.

5.1 MULTIMODAL LLMS

To validate ATOKEN‚Äôs effectiveness for vision-language understanding, we integrate it
into
SlowFast-LLaVA-1.5 (Xu et al., 2025), replacing the Oryx-ViT (Liu et al., 2024b) vision en-
coder with ATOKEN-So/C while keeping all other settings identical. To assess generalization, the
ATOKEN parameters are frozen during training, with only the SlowFast projector and LLM updated.
We evaluate using the lmms-eval (Zhang et al., 2024a) toolkit and report official metrics without
output filtering.

Image Understanding. Table 9 shows the image understanding results on 7 standard benchmarks,
including RW-QA*, AI2D (Kembhavi et al., 2016), SQA (Lu et al., 2022b), and MMMU (Yue et al.,
2024), and MathVISTA (Lu et al., 2024b) for general image QA, as well as OCRBench (Liu et al.,
2024a) and TextVQA (Singh et al., 2019) for text and document understanding. To position our
models relative to state-of-the-art methods, we compare it against LLaVA-OV (Li et al., 2024a),
MM1.5 (Zhang et al., 2025), Molmo (Deitke et al., 2024), BLIP3 (Xue et al., 2024b), Phi-3.5-
V (Abdin et al., 2024), InternVL2.5 (Zhang et al., 2024b), and Qwen2-VL (Wang et al., 2024c).

*https://huggingface.co/datasets/xai-org/RealworldQA

14

(a)(b)(c)(d)(e)Preprint

Figure 9: Qualitative comparison of image reconstruction performance across different tokenization
methods. The latent shape for a 256 √ó 256 image patch is shown under each method name. Despite operating
at higher compression ratios, ATOKEN demonstrates superior reconstruction quality, particularly excelling in
preserving high-frequency textures, fine details, and complex text elements.

Figure 10: Qualitative comparison of video reconstruction performance on 720p video sequences. The
latent shape for each video tokenization method is indicated under the method name. ATOKEN achieves com-
parable quality to specialized video-only methods while uniquely supporting both continuous and discrete
representations in a unified framework.

Figure 11: 3D Reconstruction Visualization on Toys4k. ATOKEN‚Äôs improved color consistency results in a
higher PSNR compared to specialized 3D tokenizer Trellis-SLAT.

15

Preprint

Table 9: Image understanding comparison across multimodal LLMs. Evaluation of SlowFast-LLaVA-1.5
with frozen ATOKEN-So/C vision encoder versus Oryx-ViT and other state-of-the-art MLLMs. Results shown
for 7 benchmarks (general QA and text-rich understanding) across 1B, 3B, and 7B model scales.

Vision
Encoder

# Input
Pixels

RW-QA AI2D SQA MMMU MathV

OCRBench TextVQA

(test)

(test)

(test)

(val)

(testmini)

(test)

(val)

General & Knowledge

TextRich

Multimodal LLM

1B Model Comparison
LLaVA-OV-0.5B
MM1.5-1B
MolmoE-1B
SlowFast-LLaVA-1.5-1B

SigLIP
CLIP
MetaCLIP
Oryx-ViT

SlowFast-LLaVA-1.5-1B ATOKEN-So/C

3B Model Comparison

BLIP3-4B
MM1.5-3B
Phi-3.5-V-4B
SlowFast-LLaVA-1.5-3B

SigLIP
CLIP
CLIP
Oryx-ViT

SlowFast-LLaVA-1.5-3B ATOKEN-So/C

7B Model Comparison

LLaVA-OV-7B
MM1.5-7B
Oryx1.5-7B
InternVL2.5-8B
Qwen2-VL-7B
SlowFast-LLaVA-1.5-7B

SigLIP
CLIP
Oryx-ViT
InternViT
DFN
Oryx-ViT

SlowFast-LLaVA-1.5-7B ATOKEN-So/C

5.31M
4.52M
4.10M
2.36M

2.36M

-
4.52M
-
2.36M

2.36M

5.31M
4.52M
2.36M
9.63M
-
2.36M

2.36M

55.6
53.3
60.4
59.2

60.1

60.5
56.9
-
63.4

64.3

66.3
62.5
-
70.1
70.1
67.5

68.8

57.1
59.3
86.4
72.8

74.2

-
65.7
78.1
77.0

79.1

81.4
72.2
79.7
84.5
83.0
80.4

81.2

67.2
82.1
-
87.7

88.7

88.3
85.8
91.3
90.3

89.7

96.0
89.6
-
-
-
91.1

92.1

31.4
35.8
34.9
40.5

40.6

41.1
37.1
43.0
44.7

45.7

48.8
41.8
47.1
56.0
54.1
49.0

48.7

34.8
37.2
34.0
51.0

52.5

39.6
44.4
43.9
58.6

58.4

63.2
47.6
-
64.4
58.2
62.5

61.2

-
60.5
-
70.0

67.6

-
65.7
-
73.4

73.3

-
63.5
71.3
-
-
76.4

74.5

-
72.5
78.8
71.3

72.5

71.0
76.5
72.0
73.0

72.8

-
76.5
75.7
79.1
84.3
76.4

77.7

Table 10: Video understanding performance on multimodal LLMs. Evaluation of SlowFast-LLaVA-1.5
with frozen ATOKEN-So/C vision encoder versus Oryx-ViT and other video MLLMs. Results shown for 6
benchmarks (general and long-form video understanding) across 1B, 3B, and 7B model scales.

Vision
Encoder

# Input
Tokens

VideoMME PercepTest NExT-QA LongVideoBench MLVU LVBench

(w/o sub)

(val)

(test)

(val)

(m-avg)

(avg)

General VideoQA

Long-Form Video Understanding

Multimodal LLM

1B Model Comparison

Apollo-1.5B
InternVL2.5-2B
Qwen2-VL-2B
SlowFast-LLaVA-1.5-1B

SigLIP
InternViT
DFN
Oryx-ViT

SlowFast-LLaVA-1.5-1B ATOKEN-So/C

3B Model Comparison

InternVL2-4B
LinVT-Blip3-4B
Apollo-3B
SF-LLaVA-1.5-3B

InternViT
SigLIP
SigLIP
Oryx-ViT

SF-LLaVA-1.5-3B

ATOKEN-So/C

7B Model Comparison

Oryx1.5-7B
LLaVA-Video-7B
Apollo-7B
InternVL2.5-8B
Qwen2-VL-7B
SlowFast-LLaVA-1.5-7B

Oryx-ViT
SigLIP
SigLIP
InternViT
DFN
Oryx-ViT

SlowFast-LLaVA-1.5-7B ATOKEN-So/C

3K
16K
16K
9K

9K

16K
-
3K
9K

9K

14K
11K
3K
16K
16K
9K

9K

53.0
51.9
55.6
56.6

56.7

53.9
58.3
58.4
60.8

60.4

58.8
63.3
61.3
64.2
63.3
63.9

64.5

61.0
-
53.9
61.9

63.9

53.9
-
65.0
65.8

66.0

70.0
66.9
67.3
-
62.3
69.6

70.3

-
77.2
77.2
76.7

74.8

71.1
80.1
-
80.8

80.8

81.8
83.2
-
85.0
81.2
83.3

83.7

54.1
52.0
48.7
54.3

55.1

53.0
56.6
55.1
57.2

57.2

56.3
58.2
58.5
60.0
55.6
62.5

60.6

63.3
61.4
62.7
64.3

64.7

59.9
67.9
68.7
68.8

66.7

67.5
70.8
70.9
69.0
69.8
71.5

69.8

-
37.9
39.4
39.7

41.1

35.1
-
-
43.3

41.3

39.0
-
-
43.2
44.7
45.3

44.8

Here we highlight some key observations. First, compared to Oryx-ViT, a specific vision encoder for
multimodal understanding, SlowFast-LLaVA-1.5 with ATOKEN as vision encoder shows overall bet-
ter performance on image understanding across different model scales. Specifically, Table 9 shows
that SlowFast-LLaVA-1.5-7B with ATOKEN outperforms Oryx-ViT under the same MLLM by 1.3%
on RW-QA, 1.0% on SQA, and 1.3% on TextVQA. Second, ATOKEN shows strong generalization
ability across different tasks and model scales. For reference, using ATOKEN, SlowFast-LLaVA-
1.5-3B achieves superior results on almost all benchmarks. On RW-QA and AI2D, ATOKEN out-
performs Oryx-ViT across the 1B, 3B, and 7B scales and achieves very competitive performance.

Video Understanding. The video understanding results are summarized in Table 10, covering a
range of video tasks. Video-MME (Fu et al., 2024), PercepTest (PÀòatrÀòaucean et al., 2023), and NExT-
QA (Xiao et al., 2021) assess general video QA, whereas LongVideoBench (Wu et al., 2025b),
MLVU (Zhou et al., 2024b), and LVBench (Wang et al., 2024d) focus on temporal understand-
ing on long-range context. We compared with both video specialist models, such as Apollo (Zo-

16

Preprint

Table 11: Class-conditional image generation on ImageNet 256x256. We compare different ATOKENstages
against the specialized VAVAE tokenizer using the Lightning-DiT framework. We report gFID, sFID, Inception
Score (IS), Precision (Pre.), and Recall (Rec.). ‚Ä†The VAVAE baseline applies CFG only to the first 3 latent
channels, while we follow the standard protocol of applying it to all channels.

Tokenizer

Latent
Channels

CFG
Scale

gFID‚Üì

sFID‚Üì

IS‚Üë

Pre.‚Üë Rec.‚Üë

DiT
SiT
REPA
VAVAE

ATOKEN-B/C
Stage 1
Stage 2
Stage 3

ATOKEN-So/C

Stage 1
Stage 2
Stage 3

4
4
4
32

32
48
48

32
48
48

1.5
1.5
1.35
6.7‚Ä†

1.5
1.65
1.65

1.5
1.65
1.65

2.27
2.06
1.42
1.35

1.44
1.54
1.58

1.62
1.88
1.56

4.60
4.50
4.70
4.15

4.71
4.90
4.86

4.54
4.71
4.60

278.2
270.3
305.7
295.3

273.3
254.7
254.6

253.3
231.1
260.0

0.83
0.82
0.80
0.79

0.79
0.77
0.76

0.78
0.80
0.79

0.57
0.59
0.65
0.65

0.64
0.65
0.65

0.63
0.60
0.63

har et al., 2024), LLaVA-Video (Zhang et al., 2024c), and LinVT (Gao et al., 2024), and unified
image-video MLLMs, such as Oryx1.5 (Liu et al., 2024b), InternVL2.5 (Zhang et al., 2024b), and
Qwen2VL (Wang et al., 2024c).

We outline several key observations. First, ATOKEN excels at smaller model scales. For refer-
ence, SlowFast-LLaVA-1.5-1.5B with ATOKEN achieves state-of-the-art performance on almost all
benchmarks (e.g., outperforming Oryx-ViT by 0.8% on LongVideoBench and 1.4% on LVBench).
Second, ATOKENprovides more performance gain on general video QA benchmarks. Specifically,
it achieves state-of-the-art results on VideoMME (e.g., 64.5% with 7B LLM) and PercepTest (e.g.,
70.3% with 7B LLM) across scales. Third, we note the strong performance of Oryx-ViT on long-
form video understanding, particularly on MLVU. We hypothesize that this advantage arises because
(i) Oryx-ViT was specifically designed for video understanding in LLMs and (ii) it was trained on
long-video retrieval tasks. Future work to address this gap includes incorporating more long videos
into our training data to strengthen temporal modeling over long-range context.

5.2

IMAGE GENERATION WITH CONTINUOUS TOKENS

To evaluate ATOKEN‚Äôs generative capabilities with continuous tokens, we assess class-conditional
ImageNet generation using the Lightning-DiT (Yao & Wang, 2025) framework. We compare
against both general diffusion methods (DiT (Peebles & Xie, 2022), SiT (Ma et al., 2024a)) and
reconstruction-specialized approaches (REPA (Yu et al., 2024b), VAVAE (Yao & Wang, 2025)).
For fair comparison with VAVAE ‚Äì a strong baseline optimized specifically for image reconstruc-
tion through DINOv2 alignment ‚Äì we use identical training code, only adapting the input layer for
ATOKEN‚Äôs 48-dimensional latents (vs. 32 for VAVAE).

We follow standard CFG protocol by applying guidance across all latent channels, using scale 1.65
for our 48-channel models (vs. 1.5 for 32-channel models), consistent with Lightning-DiT findings
that wider latents benefit from stronger guidance. Note that VAVAE applies CFG only to the first
three channels as reported in their work.

As shown in Table 11, ATOKEN-So/C Stage 3 achieves 1.56 gFID, competitive with specialized tok-
enizers despite optimizing for multiple modalities and tasks simultaneously. While VAVAE achieves
1.35 gFID through image-specific optimization and REPA reaches 1.42 through specialized recon-
struction alignment, ATOKEN demonstrates that unified tokenization can approach specialized per-
formance without sacrificing versatility. Notably, our Base model shows consistent performance
across stages (1.44‚Üí1.54‚Üí1.58 gFID), while the So model improves from Stage 2 to Stage 3
(1.88‚Üí1.56), suggesting that multimodal training can enhance generation quality.

17

Preprint

Table 12: Discrete Tokenizer Class-conditional Image Generation on ImageNet. We evaluate ATOKEN-
So/D against other discrete tokenizer-based generation models. Metrics include model parameters, CFG scale,
gFID, Inception Score (IS), Precision, and Recall.

Tokenizer

Generator

# Params

LFQ
TikTok-L
VQGAN
UniTok
TokenBridge

MAGVIT-V2
MaskGiT
LlamaGen
LlamaGen
TokenBridge-L

ATOKEN-So/D

TokenBridge-L

307M
227M
1.4B
1.4B
486M

548M

CFG
Scale

-
-
1.75
1
3.1

3.1

gFID‚Üì

IS‚Üë

Pre.‚Üë Rec.‚Üë

1.91
6.18
2.34
2.51
1.76

2.23

324.3
182.1
253.9
216.7
294.8

274.5

-
0.80
0.81
0.82
0.80

0.79

-
0.51
0.60
0.57
0.63

0.61

5.3

IMAGE GENERATION WITH DISCRETE TOKENS

To evaluate ATOKEN-So/D‚Äôs generative capabilities, we integrate it into the TokenBridge (Wang
et al., 2025) autoregressive framework, replacing only the tokenizer while maintaining all other set-
tings. The key architectural difference lies in token representation: TokenBridge uses 16 dimensions
with 8-level vocabularies, while ATOKEN-So/D uses 8 dimensions with 4096-level vocabularies‚Äîa
more challenging configuration that requires modeling larger discrete spaces. Additionally, Token-
Bridge employs FFT-based dimension ordering to generate low-frequency structure first, whereas
our model uses sequential generation. Following TokenBridge‚Äôs evaluation protocol, we sample
50,000 images with CFG scale 3.1.

As shown in Table 12, ATOKEN-So/D achieves a gFID of 2.23, demonstrating competitive per-
formance against specialized discrete tokenizers including LFQ (Yu et al., 2023b), TikTok-L (Yu
et al., 2024a), VQGAN (Esser et al., 2020), UniTok (Ma et al., 2025b), and TokenBridge (Wang
et al., 2025). While TokenBridge achieves lower gFID (1.76), this gap is expected given our larger
vocabulary size (4096 vs. 8) and lack of frequency-based ordering optimization. Notably, we out-
perform UniTok (2.51 gFID), the only other unified visual tokenizer, demonstrating that multimodal
capabilities need not compromise generation quality.

5.4 TEXT TO VIDEO GENERATION

To assess the text-to-video (T2V) capabilities of the ATOKEN-So/C tokenizers, we integrate them
into a video generation model. Our model is built upon the MMDiT backbone (Esser et al., 2024) and
incorporates design elements from recent video architectures (Wan et al., 2025; Kong et al., 2024;
Peng et al., 2025). Due to computational constraints, we conduct experiments with smaller models
and limited training data, maintaining consistent settings across all tokenizers for fair comparison.
Following a standard two-stage training approach, we first pretrain the model from scratch on text-
to-image (T2I) tasks with each tokenizer. We then adapt this image model for video generation,
enabling evaluation on both T2I and T2V benchmarks. To provide a fair and efficient basis for
comparing tokenizers, all training is conducted at low resolutions, using 256√ó256 for images and
192√ó336 for videos.

For T2I evaluation, we report CLIP-Score (Hessel et al., 2021), Pick-Score (Kirstain et al., 2023),
and GenEval (Ghosh et al., 2023). For T2V tasks, we evaluate performance using the VBench
benchmark (Huang et al., 2024). We compare our results against state-of-the-art video tokenizers,
namely Cosmos (Agarwal et al., 2025), Hunyuan (Kong et al., 2024), and Wan (Wan et al., 2025).
To ensure a fair comparison, we normalize the effective token budget for video generation across
all tokenizers by adjusting the patch size. For example, we use a patch size of 2√ó2 for 8√ó8 spatial
compression and 1√ó1 for 16√ó16 compression. Additionally, for T2V generation, we adjust the
classifier free guidance (CFG) scale to account for differences in channel size, using a scale of 9.0
for a channel size of 48 and 4.5 for a channel size of 16.

As shown in Table 13, our ATOKEN-So/C tokenizers achieve results comparable to specialized
video-optimized tokenizers across all metrics, outperforming Cosmos and matching the performance
of Hunyuan and Wan, even though ours are designed for a broader range of tasks.

18

Preprint

Table 13: Text-to-image and text-to-video generation benchmarks. We compare ATOKEN Stages 2-3
with specialized video tokenizers (Cosmos, Hunyuan, Wan) under resource-constrained settings. Higher scores
indicate better performance across all metrics. All models trained with identical data and model sizes for fair
comparison.

Tokenizer

Cosmos-0.1-CV4√ó8√ó8
Hunyuan
Wan2.1

ATOKEN-So/C

Stage 2
Stage 3

Comp.
Ratio

(4, 8, 8)
(4, 8, 8)
(4, 8, 8)

(4,16,16)
(4,16,16)

Latent
Size

Patch
Size

16
16
16

48
48

2
2
2

1
1

T2I

T2V: VBench

CLIP

Pick GenEval Quality

Semantic

Total

32.16
32.49
32.45

21.47
21.66
21.62

62.14% 77.27%
66.11% 79.52%
65.57% 79.74%

65.13% 74.84%
72.03% 78.02%
74.01% 78.60%

32.44
32.50

21.59
21.74

63.08% 79.30%
64.61% 79.82%

72.42% 77.92%
73.04% 78.46%

5.5

IMAGE TO 3D SYNTHESIS

To validate the utility of our learned discrete tokens for downstream generative tasks, we train an
image-to-3D synthesis model. Following the methodology of Trellis-SLAT (Xiang et al., 2024), we
adopt their diffusion model architecture and training regimen. We replace their original 3D tokens
with the tokens generated by our ATOKEN-So/C. For a fair comparison, all inference hyperparam-
eters, such as the number of diffusion steps and classifier-free guidance scale, are kept identical to
those reported in the original work.

As shown in Figure 14, our approach successfully generates 3D assets from single conditioning
images, demonstrating that our tokens are suitable for complex generative modeling. However, we
observe that the performance does not yet match the fidelity of the original Trellis-SLAT model.
Specifically, while our tokenizer demonstrates excellent reconstruction capabilities that preserve
color and structure (as in Figure 11), the generative model sometimes struggles to maintain this
consistency. The generated assets do not always adhere strictly to the color and style of the input
image.

We hypothesize that this discrepancy arises from the significantly larger latent channel dimension of
our tokenizer. ATOKEN-So/C uses 48 latent channels to accommodate rich multimodal information,
a substantial increase from the 8 channels used in Trellis-SLAT. A diffusion model operating in this
higher-dimensional space likely requires further optimization of training and inference hyperparam-
eters (e.g., conditioning strength, diffusion schedule) to leverage the conditioning signal fully. We
leave the exploration of these optimizations as a promising direction for future work.

6 RELATED WORK

Reconstruction Tokenizers. High-resolution images have been compressed using deep auto-
encoders (Hinton et al., 2012; Vincent et al., 2008), which learn lower-dimensional latent repre-
sentations for reconstruction. VAEs (Kingma & Welling, 2013) extended this framework with prob-
abilistic modeling, while VQ-VAE (Van Den Oord et al., 2017) introduced vector quantization to
discretize the latent space. Building on these foundations, subsequent works enhanced reconstruc-
tion quality through adversarial training (Rombach et al., 2022; Esser et al., 2020), developed alter-
native quantization strategies (Lee et al., 2022; Mentzer et al., 2023; Luo et al., 2024b; Zheng et al.,
2022), incorporated semantic guidance (Li et al., 2024b;c; Yao & Wang, 2025; Zha et al., 2024;
Chen et al., 2024a; 2025; Kim et al., 2025), and scaled model capacity (Xiong et al., 2025).

Video tokenization extended these image-based methods to temporal domains, employing 3D con-
volutions (Yan et al., 2021; Ge et al., 2022; Yu et al., 2023a), decoupled spatial-temporal processing
(Polyak et al., 2024), and causal modeling (Kong et al., 2024; Wan et al., 2025; Yang et al., 2024).
Beyond convolutional architectures, recent work has explored Vision Transformers (Dosovitskiy
et al., 2020) as an alternative backbone for both image (Yu et al., 2021; 2024a; Hansen-Estruch
et al., 2025) and video (Villegas et al., 2022; Wang et al., 2024b;a; Yan et al., 2024) tokenization.

3D generation methods initially applied diffusion models directly to various 3D representations
(Luo & Hu, 2021; Hui et al., 2022; Shue et al., 2023; Wang et al., 2023; He et al., 2024), then shifted
toward compact latent spaces for improved efficiency (Gupta et al., 2023; Xiong et al., 2024; Jun &
Nichol, 2023; Lan et al., 2024; Nichol et al., 2022). Notably, Trellis (Xiang et al., 2024) introduces

19

Preprint

Figure 12: ImageNet Generation Samples Using Continuous Token. Images are generated with Lightin-
ingDiT (Yao & Wang, 2025) and ATOKEN-So/C.

Figure 13: ImageNet Generation Samples Using Discrete Token. Images are generated with TokenBridge-
L (Wang et al., 2025) and ATOKEN-So/D.

Figure 14: Image-to-3D Generation Visualization on Toys4k.

20

Preprint

structured latents (SLAT) that jointly encode geometry and appearance on sparse 3D grids, enabling
flexible decoding to multiple output formats.

Visual Encoders.
Image encoders initially leveraged contrastive learning through vision-language
alignment (Radford et al., 2021; Jia et al., 2021; Zhai et al., 2023) and image-only self-supervision
(Chen et al., 2020; Oquab et al., 2023). Generative pretraining explored text generation objectives
(Wang et al., 2021), discrete token reconstruction (Bao et al., 2021), and masked image modeling
(He et al., 2022; Carreira et al., 2024). Methods like NaViT (Dehghani et al., 2023) introduced
resolution flexibility with preserved aspect ratios. Recent unified approaches merge contrastive,
generative, and self-supervised objectives (Yu et al., 2022a; Tschannen et al., 2025) or leverage
intermediate-layer features with task-specific alignment (Bolya et al., 2025).

Video encoders primarily employ self-supervised learning on video-only data (Qian et al., 2021;
Feichtenhofer et al., 2021; Recasens et al., 2021; Qian et al., 2022; Tong et al., 2022) or video-
language modeling with noisy text supervision (Fu et al., 2021; Zellers et al., 2022; Li et al., 2022;
Huang et al., 2022; Chen et al., 2023). Recent methods treat video as image sequences, focusing
on context window expansion (Team et al., 2024; Xue et al., 2024a) or token compression (Li et al.,
2023; Song et al., 2023; Fei et al., 2024; Weng et al., 2024; Xu et al., 2024).

Unified Tokenizers & Multimodal Models. Unified Multimodal Models aim to combine visual
understanding and generation within a single framework (Wang et al., 2022a; Mizrahi et al., 2023;
Lu et al., 2024a). Many approaches use decoupled tokenizers while employing various generation
paradigms ‚Äì autoregressive (Lu et al., 2022a; Team & Kahn, 2024; Wu et al., 2024a), diffusion
(Zhou et al., 2024a), flow-matching (Ma et al., 2024b), and masked prediction (Xie et al., 2024;
Tian et al., 2025). Recent efforts on unified tokenizers that handle both tasks include VILA-U
(Wu et al., 2024c), which combines pixel reconstruction with contrastive learning in a single vision
tower; SeTok (Wu et al., 2024b), which groups visual features into semantic units; UniTok (Ma et al.,
2025a), which uses multi-codebook quantization for enhanced expressiveness; and UniToken (Jiao
et al., 2025), which produces hybrid discrete-continuous representations through dual encoders.
Show-o2 (Xie et al., 2025) extends these approaches by leveraging a 3D causal VAE space with
dual-path spatial-temporal fusion, enabling scalability across both image and video modalities while
combining autoregressive modeling with flow matching.

7 DISCUSSION AND CONCLUSION

The effectiveness of ATOKEN across diverse modalities and tasks suggests new opportunities: vi-
sual tokenization can achieve the same unification that transformed language modeling. Our single
framework achieves both high-fidelity reconstruction and semantic understanding across images,
videos, and 3D assets. This integration became possible through the combination of our sparse
4D representation, transformer-based architecture, adversarial-free training strategy, and progressive
multimodal curriculum. Due to limited computational resources, we could only test ATOKENon sep-
arate downstream tasks. Building the comprehensive omnimodel that would demonstrate ATOKEN‚Äôs
full potential remains as future work. Looking forward, ATOKEN opens paths for visual foundation
models to follow language modeling‚Äôs trajectory toward true generalization. We hope this work
sheds light on the next-generation multimodal AI systems built upon unified visual tokenization.

A CONTRIBUTIONS

Jiasen designed the main concept and project scope, developed the unified representation, main
architecture, native resolution training, distill-based semantic loss, stage-wise round robin training
strategy, discrete quantization, and KV-cache video decoding, GAN training recipe etc. Curated the
image and video dataset, trained the model, conducted in-training evaluation, and wrote the paper.
Liangchen oversaw engineering aspects for the project, developed the sparse transformer structure,
adversarial-free training loss recipe, video reconstruction loss recipe, and 3D tokenizer pipeline
and dataset. Evaluated image reconstruction and understanding (Section 4.2), 3D reconstruction
and understanding (Section 4.4), ran image generation with continuous tokens (Section 5.2) and
text-to-3D synthesis (Section 5.5), and contributed to writing the paper. Mingze contributed to
the video understanding dataset design and suggested video understanding encoding settings. Ran

21

Preprint

all Multimodal LLM experiments and wrote the corresponding section (Section 5.1). Byeongjoo
contributed to the discussion of GAN settings and video reconstruction frame sampling strategy.
Evaluated video reconstruction (Section 4.3) and ran text-to-video generation experiments and wrote
the corresponding section (Section 5.4). Yanjun evaluated video retrieval (Section 4.3), ran image
generation experiments with discrete tokens, and wrote the corresponding section (Section 5.3).
Chen contributed to discussions on image understanding and image generation with continuous
tokens. Afshin advised on research direction and helped manage compute resources. Yinfei advised
on research direction, provided feedback through regular discussions, and helped manage computing
resources.

REFERENCES

Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla,
Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: A highly ca-
pable language model locally on your phone. arXiv:2404.14219, 2024.

Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report.
arXiv:2303.08774, 2023.

Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. Learning representations and
generative models for 3d point clouds. In International conference on machine learning, pp. 40‚Äì49. PMLR,
2018.

Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay,
Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world foundation model platform for physical ai.
arXiv:2501.03575, 2025.

Max Bain, Arsha Nagrani, G√ºl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder

for end-to-end retrieval. In IEEE International Conference on Computer Vision, 2021.

Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers.

arXiv:2106.08254, 2021.

Daniel Bolya, Po-Yao Huang, Peize Sun, Jang Hyun Cho, Andrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi,
Jathushan Rajasegaran, Hanoona Rasheed, et al. Perception encoder: The best visual embeddings are not at
the output of the network. arXiv:2504.13181, 2025.

Jo√£o Carreira, Dilara Gokay, Michael King, Chuhan Zhang,

Ignacio Rocco, Aravindh Mahendran,
Thomas Albert Keck, Joseph Heyward, Skanda Koppula, Etienne Pot, et al. Scaling 4d representations.
arXiv:2412.15212, 2024.

David Chen and William B Dolan. Collecting highly parallel data for paraphrase evaluation. In Proceedings of
the 49th annual meeting of the association for computational linguistics: human language technologies, pp.
190‚Äì200, 2011.

Guo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, Yifei Huang, Junting Pan, Yi Wang, Yali Wang, Y. Qiao,
Tong Lu, and Limin Wang. Videollm: Modeling video sequence with large language models. ArXiv,
abs/2305.13292, 2023.

Hao Chen, Ze Wang, Xiang Li, Ximeng Sun, Fangyi Chen, Jiang Liu, Jindong Wang, Bhiksha Raj, Zicheng
Liu, and Emad Barsoum. Softvq-vae: Efficient 1-dimensional continuous tokenizer. ArXiv, abs/2412.10958,
2024a.

Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jindong Wang, Ze Wang, Zicheng Liu, Difan Zou,
and Bhiksha Raj. Masked autoencoders are effective tokenizers for diffusion models. ArXiv, abs/2502.03444,
2025.

Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive
learning of visual representations. In International conference on machine learning, pp. 1597‚Äì1607. PmLR,
2020.

Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun
Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos
with multiple cross-modality teachers. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 13320‚Äì13331, 2024b.

22

Preprint

Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling
with pathways. Journal of Machine Learning Research, 24(240):1‚Äì113, 2023.

Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, An-
dreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim M Alabdulmohsin, et al. Patch n‚Äôpack: Navit, a
vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems,
36:2252‚Äì2274, 2023.

Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Chris-
tian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: A universe of 10m+ 3d objects.
Advances in Neural Information Processing Systems, 36:35799‚Äì35813, 2023.

Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi,
Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for
state-of-the-art multimodal models. arXiv:2409.17146, 2024.

Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiao-
nan Nie, Ziang Song, Guang Shi, and Haoqi Fan. Emerging properties in unified multimodal pretraining.
arXiv:2505.14683, 2025.

Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248‚Äì255. Ieee,
2009.

Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-
terthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
16x16 words: Transformers for image recognition at scale. arXiv:2010.11929, 2020.

Patrick Esser, Robin Rombach, and Bj√∂rn Ommer. Taming transformers for high-resolution image synthesis,

2020.

Patrick Esser, Sumith Kulal, A. Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik
Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, Kyle Lacey, Alex Good-
win, Yannik Marek, and Robin Rombach. Scaling rectified flow transformers for high-resolution image
synthesis. ArXiv, abs/2403.03206, 2024.

Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data

filtering networks. ArXiv, abs/2309.17425, 2023.

Jiajun Fei, Dian Li, Zhidong Deng, Zekun Wang, Gang Liu, and Hui Wang. Video-ccam: Enhancing video-
language understanding with causal cross-attention masks for short and long videos. ArXiv, abs/2408.14023,
2024.

Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, and Kaiming He. A large-scale study on unsu-
pervised spatiotemporal representation learning. In Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition, pp. 3299‚Äì3309, 2021.

Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou,
Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of
multi-modal llms in video analysis. arXiv:2405.21075, 2024.

Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. Violet :
End-to-end video-language transformers with masked visual-token modeling. ArXiv, abs/2111.12681, 2021.

Lishuai Gao, Yujie Zhong, Yingsen Zeng, Haoxian Tan, Dengjie Li, and Zheng Zhao. Linvt: Empower your

image-level large language model to understand videos. arXiv:2412.05185, 2024.

Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge.

Image style transfer using convolutional neural
networks. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2414‚Äì2423,
2016.

Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh.
Long video generation with time-agnostic vqgan and time-sensitive transformer. In European Conference
on Computer Vision, pp. 102‚Äì118. Springer, 2022.

Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework for evalu-

ating text-to-image alignment. NeurIPS, 2023.

23

Preprint

Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing
systems, 27, 2014.

Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma,
Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement
learning. arXiv:2501.12948, 2025.

Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Barlas OÀòguz. 3dgen: Triplane latent diffusion for

textured mesh generation. arXiv:2303.05371, 2023.

Philippe Hansen-Estruch, David Yan, Ching-Yao Chung, Orr Zohar, Jialiang Wang, Tingbo Hou, Tao Xu, Sri-
ram Vishwanath, Peter Vajda, and Xinlei Chen. Learnings from scaling visual tokenizers for reconstruction
and generation. arXiv:2501.09755, 2025.

Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll√°r, and Ross Girshick. Masked autoencoders
are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, pp. 16000‚Äì16009, 2022.

Xianglong He, Junyi Chen, Sida Peng, Di Huang, Yangguang Li, Xiaoshui Huang, Chun Yuan, Wanli Ouyang,
and Tong He. Gvgen: Text-to-3d generation with volumetric representation. In European Conference on
Computer Vision, pp. 463‚Äì479. Springer, 2024.

Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free

evaluation metric for image captioning. arXiv:2104.08718, 2021.

Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. Improving

neural networks by preventing co-adaptation of feature detectors. arXiv:1207.0580, 2012.

Jingjia Huang, Yinan Li, Jiashi Feng, Xiaoshuai Sun, and Rongrong Ji. Clover: Towards a unified video-
language alignment and fusion model. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition (CVPR), pp. 14856‚Äì14866, 2022.

Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu,
Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchmark suite for video generative
models. In CVPR, 2024.

Ka-Hei Hui, Ruihui Li, Jingyu Hu, and Chi-Wing Fu. Neural wavelet-domain diffusion for 3d shape generation.

In SIGGRAPH Asia 2022 conference papers, pp. 1‚Äì9, 2022.

Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li,
and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision.
In International conference on machine learning, pp. 4904‚Äì4916. PMLR, 2021.

Yang Jiao, Haibo Qiu, Zequn Jie, Shaoxiang Chen, Jingjing Chen, Lin Ma, and Yu-Gang Jiang. Unitoken:
Harmonizing multimodal understanding and generation through unified visual encoding. In Proceedings of
the Computer Vision and Pattern Recognition Conference, pp. 3600‚Äì3610, 2025.

Heewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. arXiv:2305.02463, 2023.

Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. A

diagram is worth a dozen images. In ECCV, 2016.

Bernhard Kerbl, Georgios Kopanas, Thomas Leimk√ºhler, and George Drettakis. 3d gaussian splatting for

real-time radiance field rendering. ACM Trans. Graph., 42(4):139‚Äì1, 2023.

Dongwon Kim, Ju He, Qihang Yu, Chenglin Yang, Xiaohui Shen, Suha Kwak, and Liang-Chieh Chen. Democ-
ratizing text-to-image masked generative models with compact text-aware one-dimensional tokens. ArXiv,
abs/2501.07730, 2025.

Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv:1312.6114, 2013.

Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An

open dataset of user preferences for text-to-image generation. NeurIPS, 2023.

Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu,
Jianwei Zhang, et al. Hunyuanvideo: A systematic framework for large video generative models.
arXiv:2412.03603, 2024.

24

Preprint

Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali,
Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari. The open images
dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV,
2020.

Black Forest Labs, Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim
Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, Kyle Lacey, Yam Levi, Cheng Li,
Dominik Lorenz, Jonas M√ºller, Dustin Podell, Robin Rombach, Harry Saini, Axel Sauer, and Luke Smith.
Flux.1 kontext: Flow matching for in-context image generation and editing in latent space, 2025.

Yushi Lan, Fangzhou Hong, Shuai Yang, Shangchen Zhou, Xuyi Meng, Bo Dai, Xingang Pan, and
Chen Change Loy. Ln3diff: Scalable latent neural fields diffusion for speedy 3d generation. In European
Conference on Computer Vision, pp. 112‚Äì130. Springer, 2024.

Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image genera-
tion using residual quantization. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 11513‚Äì11522, 2022.

Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang,

Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv:2408.03326, 2024a.

Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Ce Liu, and Lijuan Wang. Lavender: Unifying
video-language understanding as masked language modeling. 2023 IEEE/CVF Conference on Computer
Vision and Pattern Recognition (CVPR), pp. 23119‚Äì23129, 2022.

Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, and Zhe Lin. Imagefolder: Autoregres-

sive image generation with folded tokens. ArXiv, abs/2410.01756, 2024b.

Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Jiuxiang Gu, Jindong Wang, Zhe Lin, and Bhiksha Raj. Xq-gan: An
open-source image tokenization framework for autoregressive generation. ArXiv, abs/2412.01762, 2024c.

Yanwei Li, Chengyao Wang, and Jiaya Jia. Llama-vid: An image is worth 2 tokens in large language models.

In European Conference on Computer Vision, 2023.

Yuliang Liu, Zhang Li, Mingxin Huang, Biao Yang, Wenwen Yu, Chunyuan Li, Xu-Cheng Yin, Cheng-Lin Liu,
Lianwen Jin, and Xiang Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science
China Information Sciences, 2024a.

Zuyan Liu, Yuhao Dong, Ziwei Liu, Winston Hu, Jiwen Lu, and Yongming Rao. Oryx mllm: On-demand

spatial-temporal understanding at arbitrary resolution. arXiv:2409.12961, 2024b.

Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A

unified model for vision, language, and multi-modal tasks. ArXiv, abs/2206.08916, 2022a.

Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and
Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision language audio
and action. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
26439‚Äì26455, 2024a.

Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark,
and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question an-
swering. In NeurIPS, 2022b.

Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei
Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation
models in visual contexts. In ICLR, 2024b.

Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. CLIP4Clip: An empirical

study of clip for end to end video clip retrieval. arXiv:2104.08860, 2021.

Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceedings of the

IEEE/CVF conference on computer vision and pattern recognition, pp. 2837‚Äì2845, 2021.

Tiange Luo, Justin Johnson, and Honglak Lee. View selection for 3d captioning via diffusion ranking.

In

European Conference on Computer Vision, pp. 180‚Äì197. Springer, 2024a.

Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-magvit2: An open-
source project toward democratizing auto-regressive visual generation. ArXiv, abs/2409.04410, 2024b.

25

Preprint

Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi.

Unitok: A unified tokenizer for visual generation and understanding. arXiv:2502.20321, 2025a.

Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi.

Unitok: A unified tokenizer for visual generation and understanding. arXiv:2502.20321, 2025b.

Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit:
Exploring flow and diffusion-based generative models with scalable interpolant transformers. In European
Conference on Computer Vision, pp. 23‚Äì40. Springer, 2024a.

Yiyang Ma, Xingchao Liu, Xi aokang Chen, Wen Liu, Chengyue Wu, Zhiyu Wu, Zizheng Pan, Zhenda Xie,
Haowei Zhang, Xingkai Yu, Liang Zhao, Yisong Wang, Jiaying Liu, and Chong Ruan. Janusflow: Harmo-
nizing autoregression and rectified flow for unified multimodal understanding and generation. In Computer
Vision and Pattern Recognition, 2024b.

Fabian Mentzer, David C. Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization:

Vq-vae made simple. ArXiv, abs/2309.15505, 2023.

Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy
networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, pp. 4460‚Äì4470, 2019.

Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng.
Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):
99‚Äì106, 2021.

David Mizrahi, Roman Bachmann, Ouguzhan Fatih Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, and Amir

Zamir. 4m: Massively multimodal masked modeling. ArXiv, abs/2312.06647, 2023.

Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system for gener-

ating 3d point clouds from complex prompts. arXiv:2212.08751, 2022.

Maxime Oquab, Timoth√©e Darcet, Th√©o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernan-
dez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features
without supervision. arXiv:2304.07193, 2023.

William Peebles and Saining Xie.

Scalable diffusion models with transformers.

arXiv preprint

arXiv:2212.09748, 2022.

Xiangyu Peng, Zangwei Zheng, Chenhui Shen, Tom Young, Xinying Guo, Binluo Wang, Hang Xu, Hongxin
Liu, Mingyan Jiang, Wenjun Li, et al. Open-sora 2.0: Training a commercial-level video generation model
in $200 k. arXiv:2503.09642, 2025.

Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas,
Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: A cast of media foundation models.
arXiv:2410.13720, 2024.

Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbel√°ez, Alexander Sorkine-Hornung, and Luc Van

Gool. The 2017 davis challenge on video object segmentation. ArXiv, abs/1704.00675, 2017.

Viorica PÀòatrÀòaucean, Lucas Smaira, Ankush Gupta, Adri√† Recasens Continente, Larisa Markeeva, Dylan Ba-
narse, Skanda Koppula, Joseph Heyward, Mateusz Malinowski, Yi Yang, Carl Doersch, Tatiana Mate-
jovicova, Yury Sulsky, Antoine Miech, Alex Frechette, Hanna Klimczak, Raphael Koster, Junlin Zhang,
Stephanie Winkler, Yusuf Aytar, Simon Osindero, Dima Damen, Andrew Zisserman, and Jo√£o Carreira.
Perception test: A diagnostic benchmark for multimodal video models. In NeurIPS, 2023.

Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, and Yin Cui.
Spatiotemporal contrastive video representation learning. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, pp. 6964‚Äì6974, 2021.

Rui Qian, Yeqing Li, Liangzhe Yuan, Boqing Gong, Ting Liu, Matthew Brown, Serge J. Belongie, Ming-Hsuan
Yang, Hartwig Adam, and Yin Cui. On temporal granularity in self-supervised video representation learning.
In British Machine Vision Conference, 2022.

Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-
try, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural
language supervision. In International conference on machine learning, pp. 8748‚Äì8763. PmLR, 2021.

26

Preprint

Adri√† Recasens, Pauline Luc, Jean-Baptiste Alayrac, Luyu Wang, Florian Strub, Corentin Tallec, Mateusz Ma-
linowski, Viorica Patraucean, Florent Altch‚Äôe, Michael Valko, Jean-Bastien Grill, A√§ron van den Oord, and
Andrew Zisserman. Broaden your views for self-supervised video learning. 2021 IEEE/CVF International
Conference on Computer Vision (ICCV), pp. 1235‚Äì1245, 2021.

Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. High-resolution
In Proceedings of the IEEE/CVF conference on computer

image synthesis with latent diffusion models.
vision and pattern recognition, pp. 10684‚Äì10695, 2022.

Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen,
Lu Qi, Sen Wang, et al. Seaweed-7b: Cost-effective training of video generation foundation model.
arXiv:2504.08685, 2025.

Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword

units. arXiv:1508.07909, 2015.

J Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, and Gordon Wetzstein. 3d neural field
generation using triplane diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 20875‚Äì20886, 2023.

Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus

Rohrbach. Towards vqa models that can read. In CVPR, 2019.

Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang Zhou, Feiyang Wu, Xun Guo, Tianbo
Ye, Yang Lu, Jenq-Neng Hwang, and Gaoang Wang. Moviechat: From dense token to sparse memory
for long video understanding. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 18221‚Äì18232, 2023.

Stefan Stojanov, Anh Thai, and James M Rehg. Using shape to categorize: Low-shot learning with an explicit
shape bias. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.
1798‚Äì1808, 2021a.

Stefan Stojanov, Anh Thai, and James M. Rehg. Using shape to categorize: Low-shot learning with an explicit

shape bias. 2021b.

Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training techniques for

clip at scale. arXiv:2303.15389, 2023.

Chameleon Team and Jacob Kahn. Chameleon: Mixed-modal early-fusion foundation models. ArXiv,

abs/2405.09818, 2024.

Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalk-
wyk, Andrew M Dai, Anja Hauth, Katie Millican, et al. Gemini: a family of highly capable multimodal
models. arXiv:2312.11805, 2023.

Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien
Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions
of tokens of context. arXiv:2403.05530, 2024.

Rui Tian, Mingfei Gao, Mingze Xu, Jiaming Hu, Jiasen Lu, Zuxuan Wu, Yinfei Yang, and Afshin Dehghan.
Unigen: Enhanced training & test-time strategies for unified multimodal understanding and generation.
arXiv:2505.14682, 2025.

Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient

learners for self-supervised video pre-training. ArXiv, abs/2203.12602, 2022.

Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix,
Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur‚Äôelien Rodriguez, Armand Joulin,
Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. ArXiv,
abs/2302.13971, 2023.

Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin,
Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilin-
gual vision-language encoders with improved semantic understanding, localization, and dense features.
arXiv:2502.14786, 2025.

Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural infor-

mation processing systems, 30, 2017.

27

Preprint

Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang, Moham-
mad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable length video
generation from open domain textual description. arXiv:2210.02399, 2022.

Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing
robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine
learning, pp. 1096‚Äì1103, 2008.

Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao,
Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv:2503.20314,
2025.

Hanyu Wang, Saksham Suri, Yixuan Ren, Hao Chen, and Abhinav Shrivastava. Larp: Tokenizing videos with

a learned autoregressive generative prior. arXiv:2410.21264, 2024a.

Junke Wang, Yi Jiang, Zehuan Yuan, Bingyue Peng, Zuxuan Wu, and Yu-Gang Jiang. Omnitokenizer: A
joint image-video tokenizer for visual generation. Advances in Neural Information Processing Systems, 37:
28281‚Äì28295, 2024b.

Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren
Zhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-
sequence learning framework. In International Conference on Machine Learning, 2022a.

Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang,
Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language model‚Äôs perception of the world at any resolution.
arXiv:2409.12191, 2024c.

Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong
Chen, Fang Wen, Qifeng Chen, et al. Rodin: A generative model for sculpting 3d digital avatars using
In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.
diffusion.
4563‚Äì4573, 2023.

Weihan Wang, Zehai He, Wenyi Hong, Yean Cheng, Xiaohan Zhang, Ji Qi, Xiaotao Gu, Shiyu Huang, Bin Xu,
Yuxiao Dong, et al. Lvbench: An extreme long video understanding benchmark. arXiv:2406.08035, 2024d.

Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu,
Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, and Yu Qiao. Intern-
video: General video foundation models via generative and discriminative learning. ArXiv, abs/2212.03191,
2022b.

Yuqing Wang, Zhijie Lin, Yao Teng, Yuanzhi Zhu, Shuhuai Ren, Jiashi Feng, and Xihui Liu. Bridging contin-

uous and discrete tokens for autoregressive visual generation. arXiv:2503.16430, 2025.

Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual

language model pretraining with weak supervision. arXiv:2108.10904, 2021.

Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long video

understanding via large language models. In European Conference on Computer Vision, 2024.

Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao

Xu, Yilei Chen, et al. Qwen-image technical report, 2025a.

Chengyue Wu, Xi aokang Chen, Zhiyu Wu, Yiyang Ma, Xingchao Liu, Zizheng Pan, Wen Liu, Zhenda Xie,
Xingkai Yu, Chong Ruan, and Ping Luo. Janus: Decoupling visual encoding for unified multimodal under-
standing and generation. ArXiv, abs/2410.13848, 2024a.

Haoning Wu, Dongxu Li, Bei Chen, and Junnan Li. Longvideobench: A benchmark for long-context inter-

leaved video-language understanding. NeurIPS, 2025b.

Shengqiong Wu, Hao Fei, Xiangtai Li, Jiayi Ji, Hanwang Zhang, Tat-Seng Chua, and Shuicheng Yan. Towards

semantic equivalence of tokenization in multimodal llm. arXiv:2406.05127, 2024b.

Weijia Wu, Yuzhong Zhao, Zhuang Li, Jiahong Li, Hong Zhou, Mike Zheng Shou, and Xiang Bai. A large
cross-modal video retrieval dataset with reading comprehension. Pattern Recognition, 157:110818, 2025c.

Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie,
Hongxu Yin, Li Yi, et al. Vila-u: a unified foundation model integrating visual understanding and generation.
arXiv:2409.04429, 2024c.

28

Preprint

Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and
Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. arXiv:2412.01506, 2024.

Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. NExT-QA: Next phase of question-answering to

explaining temporal actions. In CVPR, 2021.

Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu,
Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal
understanding and generation. ArXiv, abs/2408.12528, 2024.

Jinheng Xie, Zhenheng Yang, and Mike Zheng Shou. Show-o2: Improved native unified multimodal models.

ArXiv, abs/2506.15564, 2025.

Bojun Xiong, Si-Tong Wei, Xin-Yang Zheng, Yan-Pei Cao, Zhouhui Lian, and Peng-Shuai Wang. Octfusion:

Octree-based diffusion models for 3d shape generation. arXiv:2408.14732, 2024.

Tianwei Xiong, Jun Hao Liew, Zilong Huang, Jiashi Feng, and Xihui Liu. Gigatok: Scaling visual tokenizers

to 3 billion parameters for autoregressive image generation. arXiv:2504.08736, 2025.

Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao (Bernie) Huang, Russell Howes, Vasu Sharma, Shang-
Wen Li, Gargi Ghosh, Luke S. Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. ArXiv,
abs/2309.16671, 2023.

Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and
language. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5288‚Äì5296,
2016.

Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, and Afshin De-
hghan. Slowfast-llava: A strong training-free baseline for video large language models. arXiv:2407.15841,
2024.

Mingze Xu, Mingfei Gao, Shiyu Li, Jiasen Lu, Zhe Gan, Zhengfeng Lai, Meng Cao, Kai Kang, Yinfei Yang,
and Afshin Dehghan. Slowfast-llava-1.5: A family of token-efficient video large language models for long-
form video understanding. arXiv:2503.18943, 2025.

Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang,
Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu,
Yao Lu, and Song Han. Longvila: Scaling long-context visual language models for long videos. ArXiv,
abs/2408.10188, 2024a.

Le Xue, Manli Shu, Anas Awadalla, Jun Wang, An Yan, Senthil Purushwalkam, Honglu Zhou, Viraj Prabhu,
Yutong Dai, Michael S Ryoo, et al. xgen-mm (blip-3): A family of open large multimodal models.
arXiv:2408.08872, 2024b.

Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using vq-vae and

transformers. arXiv:2104.10157, 2021.

Wilson Yan, Matei Zaharia, Volodymyr Mnih, Pieter Abbeel, Aleksandra Faust, and Hao Liu. Elastictok:

Adaptive tokenization for image and video. ArXiv, abs/2410.08368, 2024.

Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi
Hong, Xiaohan Zhang, Guanyu Feng, Da Yin, Xiaotao Gu, Yuxuan Zhang, Weihan Wang, Yean Cheng,
Ting Liu, Bin Xu, Yuxiao Dong, and Jie Tang. Cogvideox: Text-to-video diffusion models with an expert
transformer. ArXiv, abs/2408.06072, 2024.

Jingfeng Yao and Xinggang Wang. Reconstruction vs. generation: Taming optimization dilemma in latent

diffusion models. ArXiv, abs/2501.01423, 2025.

Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason
Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv:2110.04627,
2021.

Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Con-

trastive captioners are image-text foundation models. arXiv:2205.01917, 2022a.

Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos√© Lezama, Han Zhang, Huiwen Chang, Alexander G. Hauptmann,
Ming-Hsuan Yang, Yuan Hao, Irfan Essa, and Lu Jiang. Magvit: Masked generative video transformer. 2023
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 10459‚Äì10469, 2022b.

29

Preprint

Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos√© Lezama, Han Zhang, Huiwen Chang, Alexander G Hauptmann,
Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10459‚Äì10469, 2023a.

Lijun Yu, Jos√© Lezama, Nitesh Bharadwaj Gundavarapu, Luca Versari, Kihyuk Sohn, David C. Minnen, Yong
Cheng, Agrim Gupta, Xiuye Gu, Alexander G. Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa,
David A. Ross, and Lu Jiang. Language model beats diffusion ‚Äì tokenizer is key to visual generation. 2023b.

Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is
worth 32 tokens for reconstruction and generation. Advances in Neural Information Processing Systems, 37:
128940‚Äì128966, 2024a.

Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie.
Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv
preprint arXiv:2410.06940, 2024b.

Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang,
Weiming Ren, et al. Mmmu: A massive multi-discipline multimodal understanding and reasoning bench-
mark for expert agi. In CVPR, 2024.

Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya Kusupati,
Jack Hessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Neural script knowledge through vision and
language and sound. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.
16354‚Äì16366, 2022.

Kaiwen Zha, Lijun Yu, Alireza Fathi, David A. Ross, Cordelia Schmid, Dina Katabi, and Xiuye Gu. Language-

guided image tokenization for generation. ArXiv, abs/2412.05796, 2024.

Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-
training. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 11941‚Äì11952, 2023.

Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du,
Bowen Zhang, Yanghao Li, et al. MM1. 5: Methods, analysis & insights from multimodal llm fine-tuning.
ICLR, 2025.

Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan
Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmms-eval: Reality check on the evaluation of large
multimodal models, 2024a.

Pan Zhang, Xiaoyi Dong, Yuhang Zang, Yuhang Cao, Rui Qian, Lin Chen, Qipeng Guo, Haodong Duan, Bin
Wang, Linke Ouyang, et al. Internlm-xcomposer-2.5: A versatile large vision language model supporting
long-contextual input and output. arXiv:2407.03320, 2024b.

Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness

of deep features as a perceptual metric. In CVPR, 2018.

Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning

with synthetic data. arXiv:2410.02713, 2024c.

Long Zhao, Nitesh B Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer J Sun, Luke Friedman, Rui
Qian, Tobias Weyand, Yue Zhao, et al. Videoprism: A foundational visual encoder for video understanding.
2024.

Chuanxia Zheng, Long Tung Vuong, Jianfei Cai, and Dinh Q. Phung. Movq: Modulating quantized vectors for

high-fidelity image generation. ArXiv, abs/2209.09002, 2022.

Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn,
Xuezhe Ma, Luke S. Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images
with one multi-modal model. ArXiv, abs/2408.11039, 2024a.

Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Shitao Xiao, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun
Huang, and Zheng Liu. Mlvu: A comprehensive benchmark for multi-task long video understanding.
arXiv:2406.04264, 2024b.

Orr Zohar, Xiaohan Wang, Yann Dubois, Nikhil Mehta, Tong Xiao, Philippe Hansen-Estruch, Licheng Yu,
Xiaofang Wang, Felix Juefei-Xu, Ning Zhang, et al. Apollo: An exploration of video understanding in large
multimodal models. arXiv:2412.10360, 2024.

30

